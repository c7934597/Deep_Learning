{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image_Classification_Pytorch (Training)\nhttps://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html","metadata":{}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# Table of Contents\n\n1. [套件安裝與載入](#1)\n1. [環境檢測與設定](#2)\n1. [開發參數設定](#3)\n1. [資料處理](#4)\n    -  [載入CSV檔](#4.1)\n    -  [檢查CSV檔缺失值](#4.2)\n1. [定義模型方法](#5)\n1. [定義回調函數方法](#6)\n1. [製作資料集＆資料擴增&回調函數&訓練模型](#7)\n1. [混淆矩陣](#8)\n1. [待辦事項](#9)","metadata":{}},{"cell_type":"markdown","source":"# 1. 套件安裝與載入<a class=\"anchor\" id=\"1\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"!pip3 install git+https://github.com/rwightman/pytorch-image-models.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 資料處理套件\nimport os\nimport cv2\nimport sys\nimport time\nimport timm\nimport copy\nimport random\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport albumentations as A\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 設定顯示中文字體\nfrom matplotlib.font_manager import FontProperties\nplt.rcParams['font.sans-serif'] = ['Microsoft JhengHei'] # 用來正常顯示中文標籤\nplt.rcParams['font.family'] = 'AR PL UMing CN'\nplt.rcParams['axes.unicode_minus'] = False # 用來正常顯示負號","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pytorch深度學習模組套件\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torch.nn.functional as F\nimport torchvision\n\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torchvision import models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. 環境檢測與設定<a class=\"anchor\" id=\"2\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDEVICE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''執行環境參數設定'''\n\n# (Boolean)是否為本機\nLOCAL = False\n\n# (Boolean)是否為 Colab\nCOLAB = False\n\n\n'''檔案路徑參數設定'''\n\n# (String)Root路徑\nif LOCAL:\n    PATH = r'../'\nelif COLAB:\n    PATH = r'/content/drive/My Drive/Colab Notebooks/'\nelse:\n    PATH = r'../input/'\n    OUTPUT_PATH = r'/kaggle/working/'\n    \n# (String)資料根路徑\nDATA_ROOT_PATH = PATH+r'aptos2019-blindness-detection/' \n\n# (String)訓練資料路徑\nTRAIN_DATA_PATH = DATA_ROOT_PATH+r'train_images/'\n\n# (String)訓練CSV路徑，如為None則不讀CSV檔\nTRAIN_CSV_PATH = DATA_ROOT_PATH+r'train.csv'\n\n# (String)專案名稱\nPROJECT_NAME = 'Blindness Detection'\n\n# (String)專案檔案儲存路徑\nif LOCAL or COLAB:\n    OUTPUT_PATH = PATH\nPROJECT_PATH = OUTPUT_PATH+PROJECT_NAME+'/'+PROJECT_NAME+' '+datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n\n# (String)權重名稱(使用哪個權重)\nWEIGHTS_NAME = 'resnet18'\n\n# (String)模型名稱(使用哪個模型)\nMODEL_NAME = 'resnet18'\n\n# (String)讀取預訓練權重的儲存路徑 \nLOAD_WEIGHTS_PATH = PROJECT_PATH+r'/models/backup/'+WEIGHTS_NAME+'.pth'\n\n# (String)讀取預訓練模型的儲存路徑 \nLOAD_MODEL_PATH = PROJECT_PATH+r'/models/backup/'+MODEL_NAME+'.pth'\n\n# (String)訓練模型的儲存路徑\nTRAIN_MODEL_PATH = PROJECT_PATH+r'/models/'+MODEL_NAME+'.pth'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not LOCAL and COLAB:\n    from google.colab import drive\n    drive.mount('/content/drive')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEVICE != \"CPU\":\n    !nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.isfile(TRAIN_CSV_PATH):\n    LOAD_CSV = True\nelse:\n    LOAD_CSV = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.isdir(PROJECT_PATH+r'/models/'):\n    os.makedirs(PROJECT_PATH+r'/models/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. 開發參數設定<a class=\"anchor\" id=\"3\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"'''客製參數設定'''\n\n\n'''資料參數設定'''\n\n# (Int)分類數量\nCLASSES = 5\n\n# (Int)有CSV檔該參數才有用，1則為不做交叉驗證\nFOLD = 1\n\n# (Int)沒CSV檔，FOLD該參數固定為1\nif not LOAD_CSV:\n    FOLD = 1\n    \n# (Int)圖片尺寸\nIMAGE_SIZE = [224]*FOLD\n\n# (String)圖片副檔名\nIMAGE_NAME_EXTENSION = '.png'\n\n# (String)CSV圖片檔名欄位\nIMAGE_NAME = 'id_code'\n\n# (String)CSV標籤欄位\nLABEL_NAME = 'diagnosis'\n\n# (String)CSV標籤欄位類型\nLABEL_NAME_TYPE = 'string'\n\n# (Boolean)CSV圖片檔名欄位是否包含副檔名\nIMAGE_NAME_HAVE_EXTENSION = False\n\n#  (Boolean)圖像轉為RGB\nCOLOR_CONVERT_RGB = True\n\n# (Int)不同的種子會產生不同的Random或分層K-FOLD分裂, 42則是預設固定種子\nSEED = 42\n\nif FOLD == 1:\n    # (Float)驗證集佔訓練集的比率，FOLD>1則不啟用\n    DATA_SPLIT = 0.2\nelse:\n    # (String)切分訓練集跟驗證集方式\n    SKF = StratifiedKFold(n_splits=FOLD,shuffle=True,random_state=SEED)\n\n# (Boolean)是否資料轉Tensor時啟動鎖頁內存(GPU內存)，而不鎖頁內存就是會使用到硬碟虛擬內存\nPIN_MEMORY = False\n\n# (Int)要用於數據加載的子進程數。0表示將在主進程中加載數據。（默認值：0）\nNUM_WORKERS = 0\n\n# (Boolean)批次處理在大小不合適的情況下，是否刪除最後一個不完整的批次\nDROP_LAST = False\n    \n# (Boolean)如為True每次返回的卷積算法將是確定的，即默認算法\nCUDNN_DETERMINISTIC = True\n\n# (Boolean)PyTorch 中對模型裡的卷積層進行預先的優化，也就是在每一個卷積層中測試 cuDNN 提供的所有卷積實現算法，\n# 然後選擇最快的那個。這樣在模型啟動的時候，只要額外多花一點點預處理時間，就可以較大幅度地減少訓練時間\nCUDNN_BENCHMARK = True\n\n\n'''資料擴增參數設定\n\n資料擴增範例\nhttps://zh-hant.hotbak.net/key/albumentation%E6%95%B8%E6%93%9A%E5%A2%9E%E5%BC%B7CSDN.html\n\n資料擴增教學\nhttps://zhuanlan.zhihu.com/p/107399127\n\n資料擴增Doc\nhttps://vfdev-5-albumentations.readthedocs.io/en/docs_pytorch_fix/api/augmentations.html\n'''\n\n# (Float)訓練集資料擴增的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_TRAIN_TRANSFORMS = 1.0\n\n# (Float)驗證集資料擴增的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_VAL_TRANSFORMS = 1.0\n\n# 以下資料擴增為訓練集使用=============================================\n\n# (Float)模糊的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_BLUR = 0\n\n# (Int)模糊的上限\nBLUR_LIMIT = 3\n\n# (Float)水平翻轉的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_HORIZONTALFLIP = 0\n\n# (Float)垂直翻轉的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_VERTICALFLIP = 0\n\n# (Float)水平和垂直翻轉的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_FLIP = 0\n\n# (Float)隨機旋轉90度的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_RANDOMROTATE90 = 0\n\n# (Float)平移縮放旋轉的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_SHIFTSCALEROTATE = 0\n\n# (Float)平移縮放旋轉的平移上限\nSHIFTSCALEROTATE_SHIFT_LIMIT = 0.0625\n\n# (Float)平移縮放旋轉的縮放上限\nSHIFTSCALEROTATE_SCALE_LIMIT = 0.1\n\n# (Float)平移縮放旋轉的旋轉上限\nSHIFTSCALEROTATE_ROTATE_LIMIT = 45\n\n# (Float)彈性變換的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_ELATICTRANSFORM = 0\n\n# (Float)彈性變換的alpha高斯過濾參數\nELATICTRANSFORM_ALPHA = 1\n\n# (Float)彈性變換的sigma高斯過濾參數\nELATICTRANSFORM_SIGMA = 50\n\n# (Float)彈性變換的alpha_affine，範圍為（-alpha_affine，alpha_affine）\nELATICTRANSFORM_ALPHA_AFFINE = 50\n\n# (Float)網格失真的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_GRIDDISTORTION = 0\n\n# (Int)網格失真的每一條邊上網格單元數量\nGRIDDISTORTION_NUM_STEPS = 5\n\n# (Float)隨機亮度對比度的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_RANDOMBRIGHTNESSCONTRAST_CONTRAST = 0\n\n# (Float)隨機亮度的上限\nRANDOMBRIGHTNESSCONTRAST_BRIGHTNESS_LIMIT = 0.2\n\n# (Float)隨機對比度的上限\nRANDOMBRIGHTNESSCONTRAST_CONTRAST_LIMIT = 0.2\n\n# (Float)隨機色調飽和度的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_HUESATURATIONVALUE = 0\n\n# (Float)隨機色調飽和度的色調上限\nHUESATURATIONVALUE_HUE_SHIFT_LIMIT = 20\n\n# (Float)隨機色調飽和度的飽和度上限\nHUESATURATIONVALUE_SAT_SHIFT_LIMIT = 30\n\n# (Float)隨機色調飽和度的值上限\nHUESATURATIONVALUE_VAL_SHIFT_LIMIT = 20\n\n# (Float)對比度受限自適應直方圖均衡的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_CLAHE = 0\n\n# (Float)對比度受限自適應直方圖均衡的對比度上限\nCLAHE_CLIP_LIMIT = 4.0\n\n# (Float)隨機在圖像上生成黑色矩形的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_COARSEDROPOUT = 0\n\n# (Int)隨機在圖像上生成黑色矩形的數量\nCOARSEDROPOUT_NUM_HOLES = 8\n\n# (Int)隨機在圖像上生成黑色矩形的最大高度\nCOARSEDROPOUT_MAX_H_SIZE = 8\n\n# (Int)隨機在圖像上生成黑色矩形的最大寬度\nCOARSEDROPOUT_MAX_W_SIZE = 8\n\n# (Float)隨機縮放剪裁的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_RANDOMRESIZEDCROP = 0\n\n# (Float Tuple)隨機縮放剪裁之前的圖像比例縮放\nRANDOMRESIZEDCROP_SCALE = (0.08, 1.0)\n\n# (Int)隨機縮放剪裁之前的圖像高度\nRANDOMRESIZEDCROP_HEIGHT = IMAGE_SIZE[0]\n\n# (Int)隨機縮放剪裁之前的圖像寬度\nRANDOMRESIZEDCROP_WIDTH = IMAGE_SIZE[0]\n\n# 以上資料擴增為訓練集使用=============================================\n\n# 以下資料擴增為訓練集和驗證集共用======================================\n\n# (Float)縮放的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_RESIZE = 1.0\n\n# (Int)縮放後的圖片高度\nRESIZE_HEIGHT = IMAGE_SIZE[0]\n\n# (Int)縮放後的圖片寬度\nRESIZE_WIDTH = IMAGE_SIZE[0]\n\n# (Float)正規化的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_NORMALIZE = 1.0\n\n# (List)正規化的平均值(Imagenet的參考平均值[0.485, 0.456, 0.406])\nNORMALIZE_MEAN = [0.485, 0.456, 0.406]\n\n# (List)正規化的標準差(Imagenet的參考標準差[0.229, 0.224, 0.225])\nNORMALIZE_STD = [0.229, 0.224, 0.225]\n\n# (Float)正規化的PIXEL最大值(Imagenet的參考PIXEL最大值255.0)\nNORMALIZE_MAX_PIXEL_VALUE = 255.0\n\n# (Float)歸一化的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\n# ToTensorV2()將[0, 255]的PIL.Image或[H, W, C]的numpy.ndarray數據，\n# 轉換為形狀[C, H, W]的torch.FloadTensor，並歸一化到[0, 1.0]。\nP_TOTENSORV2 = 1.0\n\n# 以上資料擴增為訓練集和驗證集共用======================================\n\n\n''''模型參數設定'''\n\n# (Boolean)使用基礎模型，如為False則須客制另外撰寫\nUSE_BASE_MODEL = True\n\nif USE_BASE_MODEL:\n    # (Boolean)使用基礎timm模型，如為False則使用基礎Pytorch模型\n    USE_BASE_TIMM_MODEL = False\n    if USE_BASE_TIMM_MODEL:\n        # (Model)建立timm模型\n        BASE_MODEL = \"tf_efficientnet_b4_ns\"\n    else:\n        # (Model)建立Pytorch模型\n        BASE_MODEL = models.resnet18\n    \n# (Boolean)是否使用基礎模型權重\nLOAD_BASE_WEIGHTS = True\n\n# (Boolean)基礎模型是否包含完全連接網路頂部的網路層\nINCLUDE_TOP = False\n\n# (Boolean)基礎模型是否可訓練權重(不包括頂部網路層)\nBASE_MODEL_TRAINABLE = False\n\n# (Boolean)是否已有客製模型，僅載入權重\nLOAD_WEIGHTS = False\n\n# (Boolean)是否載入完整客製(模型+權重)\nLOAD_MODEL = False\n\n# (Float)Dropout比率 0.5\nDROPOUT = 0.5\n\n# (Boolean)Bias偏移量\nBIAS = True\n\n# (Boolean)是否印出完整模型\nMODEL_PRINT = False\n\n\n''''回調函數參數設定\n\n學習率遞減\nhttps://zhuanlan.zhihu.com/p/69411064\n\n回調函數Doc\nhttps://pytorch.org/docs/stable/optim.html\n\n模型儲存\nhttps://pytorch.org/tutorials/beginner/saving_loading_models.html\n\n'''\n\n# (Boolean)回調函數 ModelCheckpoint 是否啟用\nCALLBACKS_CHECK_POINTER = True\n\n# (Boolean)回調函數 StepLR 是否啟用\nCALLBACKS_STEPLR = True\n\n# (Boolean)回調函數 ReduceLROnPlateau 是否啟用\nCALLBACKS_REDUCELRONPLATEAU = False\n\n# (Boolean)回調函數 CosineAnnealingWarmRestarts 是否啟用\nCALLBACKS_COSINEANNEALINGWARMRESTAERS = False\n\n# (String)回調函數監控數值(val_auc 僅限雙分類) val_acc/val_loss/val_auc\nMONITOR = 'val_loss'\n\n# (Boolean)回調函數 ModelCheckpoint 是否只儲存最佳模型 False\nSAVE_BEST_ONLY = True\n\n# (Boolean)回調函數 ModelCheckpoint 是否只儲存權重 True\nSAVE_WEIGHTS_ONLY = True\n\n# (Int)學習率衰減的時間段\nSTEP_SIZE = 15\n\n# (Float)學習率衰減的乘數 0.1\nGAMMA = 0.1\n\n# (String)最小，最大之一 在最小模式下，當監視的數量停止減少時，lr將減小； 在最大模式下，當監視的數量停止增加時，它將減少。 min\nMODE = \"min\"\n\n# (Float)學習率降低的因數。new_lr = lr *因子 0.1\nFACTOR = 0.1\n\n# (Int)沒有改善的時期數，此後學習率將降低。例如，如果 耐心= 2，那麼我們將忽略前兩個時期而沒有任何改善，\n# 並且如果損失仍然沒有改善，則只會在第三個時期之後降低LR。 10\nPATIENCE = 10 \n\n# (Float)用於測量新的最佳閾值，僅關注重大變化。 1e-4\nTHRESHOLD = 1e-4 \n\n# (String)rel，abs之一。在rel模式下，“ max”模式下的dynamic_threshold = best *（1 +閾值），在min模式下，\n# dynamic_threshold = best *（1-threshold）。在絕對模式下，dynamic_threshold =最佳+ 最大模式下的閾值或最佳-最小模式下的閾值。 rel\nTHRESHOLD_MODE = \"rel\"\n\n# (Int)減少lr後恢復正常運行之前要等待的時期數。 0 \nCOOLDOWN = 0\n\n# (Float/List)標量或標量列表。所有參數組或每個組的學習率的下限。 0 \nMIN_LR = 0\n\n# (Float)應用於lr的最小衰減。如果新舊lr之間的差異小於eps，則忽略該更新。 1e-8\nSCHEDULER_EPS = 1e-8\n\n# (Int)第一次重啟的迭代次數。\nT_0 = 15\n\n# (Int)重新啟動後，因素增加。 1 \nT_MULT = 1\n\n# (Int)最低學習率。 0\nETA_MIN = 0\n\n# (Boolean)每次更新都會向輸出印出一條消息。 False\nSCHEDULER_VERBOSE = False\n\n# (Boolean)訓練集每批就更新回調函式，否則每時代就更新。 False\nSCHEDULER_BATCH_UPDATE = False\n\n# (Boolean)驗證集回調函式是否啟用。 False\nVAL_ENABLE_SCHEDULER = False\n\n# (Boolean)驗證集通過計算LOSS就更新回調函式，否則不計算就更新。 False\nSCHEDULER_LOSS_UPDATE = False\n\n\n''''編譯參數設定\n\n編譯參數Doc\nhttps://pytorch.org/docs/stable/optim.html\n\n'''\n\n# (String)優化器指定(SGD/Adam/Adamax/RMSprop/Adagrad)，None為客制，須另外撰寫\nOPTIMIZERS_TYPE = \"Adam\"\n\n# (Float)優化器學習率 1e-3/1e-1\nLEARNING_RATE = 1e-3\n\n# (Float)學習速率衰減 0\nLR_DECAY = 0\n\n# (Float)優化器權重衰減 5e-5/5e-4\nWEIGHT_DECAY = 5e-5\n\n# (Float)加速優化器在相關方向上前進，並抑制震盪 0.9\nMOMENTUM = None\n\n# (Tuple Float)用於計算梯度及其平方的移動平均值的係數 (0.9, 0.999)\nBETAS = (0.9, 0.999)\n\n# (Float)分母中添加的項，以提高數值穩定性 1e-8\nEPS = 1e-8\n\n# (Float)平滑常數 0.99\nALPHA = 0.99\n\n# (Boolean)計算居中RMSProp，則通過估計其方差來對梯度進行歸一化 True\nCENTERED = True\n\n# (Float)阻尼動量 0\nDAMPENING = 0\n\n# (Boolean)啟用Nesterov動量 False\nNESTEROV = False\n\n# (String)損失函數，None為客制，須另外撰寫\nBASE_LOSSES = nn.CrossEntropyLoss\n\n# (String)指定還原成適用於輸出，預設mean\nREDUCTION = \"mean\"\n\n# (Boolean)是否印出完整編譯器\nOPTIMIZER_PRINT = False\n\n\n''''訓練參數設定'''\n\n# (Int List)每批訓練的尺寸\nBATCH_SIZE = [512]*FOLD\n\n# (Int)訓練做幾次時代\nEPOCHS = [2]*FOLD\n\n# (Int)指定列印進度條的位置（從0開始）。\nTQDM_POSITION = 0\n\n# (Boolean)保留迭代結束時進度條的所有痕跡。如果是None，只會在position是0時離開\nTQDM_LEAVE = True\n\n# https://kozodoi.me/python/deep%20learning/pytorch/tutorial/2021/02/19/gradient-accumulation.html\n# (Int)假設您要在一批中使用32張圖像，但是一旦超出8張，硬件就會崩潰。\n# 在這種情況下，您可以使用8張圖像的批次並每4批次更新一次權重，因此設置 4。\nACCUM_ITER = 1\n\n\n''''圖表參數設定'''\n\n# (Float)全部SNS圖表的字形縮放\nALL_SNS_FONT_SCALE = 1.0\n\n# (Int)CSV缺失值圖表寬度\nCSV_COUNTPLOT_FIGSIZE_W = 10\n\n# (Int)CSV缺失值圖表高度\nCSV_COUNTPLOT_FIGSIZE_H = 10\n\n# (Int)CSV缺失值圖表標題字型大小\nCSV_COUNTPLOT_TITLE_FONTSIZE = 20\n\n# (Int)CSV缺失值圖表X軸標題字型大小\nCSV_COUNTPLOT_XLABEL_FONTSIZE = 15\n\n# (Int)CSV缺失值圖表Y軸標題字型大小\nCSV_COUNTPLOT_YLABEL_FONTSIZE = 15","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = CUDNN_DETERMINISTIC\n    torch.backends.cudnn.benchmark = CUDNN_BENCHMARK\n\nseed_everything(SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 設置sns圖表縮放係數\nsns.set(font_scale = ALL_SNS_FONT_SCALE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. 資料處理<a class=\"anchor\" id=\"4\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"markdown","source":"## 4.1 載入CSV檔 <a class=\"anchor\" id=\"4.1\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"if LOAD_CSV:\n    print('Reading data...')\n\n    # 讀取訓練資料集CSV檔\n    train_csv = pd.read_csv(TRAIN_CSV_PATH,encoding=\"utf8\")\n\n    print('Reading data completed')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if LOAD_CSV:\n    # 顯示訓練資料集CSV檔\n    print(train_csv.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if LOAD_CSV:\n    print(\"Shape of train_data :\", train_csv.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 檢查CSV檔缺失值 <a class=\"anchor\" id=\"4.2\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"# 缺失值比率\nif LOAD_CSV:\n    total = train_csv.isnull().sum().sort_values(ascending = False)\n    percent = (train_csv.isnull().sum()/train_csv.isnull().count()*100).sort_values(ascending = False)\n    missing_train_csv  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    print(missing_train_csv.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if LOAD_CSV:\n    print(train_csv[LABEL_NAME].value_counts())\n    f,ax = plt.subplots(figsize=(CSV_COUNTPLOT_FIGSIZE_W, CSV_COUNTPLOT_FIGSIZE_H))\n    sns.countplot(train_csv[LABEL_NAME], hue = train_csv[LABEL_NAME],ax = ax)\n    plt.title(\"LABEL COUNT\", fontsize=CSV_COUNTPLOT_TITLE_FONTSIZE)\n    plt.xlabel(LABEL_NAME.upper(), fontsize=CSV_COUNTPLOT_XLABEL_FONTSIZE)\n    plt.ylabel(\"COUNT\", fontsize=CSV_COUNTPLOT_YLABEL_FONTSIZE)\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. 定義模型方法<a class=\"anchor\" id=\"5\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"def build_optimizers(model):\n    if OPTIMIZERS_TYPE == None:\n        print(\"Custiom OPTIMIZERS\")\n    elif OPTIMIZERS_TYPE == \"SGD\":\n        RETURN_OPTIMIZERS = optim.SGD(model.parameters(), \n                                       lr = LEARNING_RATE, momentum = MOMENTUM, \n                                      dampening = DAMPENING, weight_decay = WEIGHT_DECAY, \n                                      nesterov = NESTEROV)\n    elif OPTIMIZERS_TYPE == \"Adam\":\n        RETURN_OPTIMIZERS = optim.Adam(model.parameters(), \n                                       lr = LEARNING_RATE, betas = BETAS, eps = EPS, \n                                       weight_decay = WEIGHT_DECAY)\n    elif OPTIMIZERS_TYPE == \"Adamax\":\n        RETURN_OPTIMIZERS = optim.Adamax(model.parameters(), \n                                         lr = LEARNING_RATE, betas = BETAS, eps = EPS, \n                                         weight_decay = WEIGHT_DECAY)\n    elif OPTIMIZERS_TYPE == \"RMSprop\":\n        RETURN_OPTIMIZERS = optim.RMSprop(model.parameters(), \n                                          lr = LEARNING_RATE, alpha = ALPHA, eps = EPS, \n                                          weight_decay = WEIGHT_DECAY, momentum = MOMENTUM, \n                                          centered = CENTERED)\n    elif OPTIMIZERS_TYPE == \"Adagrad\":\n        RETURN_OPTIMIZERS = optim.Adagrad(model.parameters(), \n                                          lr = LEARNING_RATE, lr_decay = LR_DECAY, \n                                          weight_decay = WEIGHT_DECAY)\n    return RETURN_OPTIMIZERS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reference: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/173733\nclass MyCrossEntropyLoss(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean'):\n        super().__init__(weight=weight, reduction=reduction)\n        self.weight = weight\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        lsm = F.log_softmax(inputs, -1)\n\n        if self.weight is not None:\n            lsm = lsm * self.weight.unsqueeze(0)\n\n        loss = -(targets * lsm).sum(-1)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n    \ndef build_losses():\n    if BASE_LOSSES == None:\n        RETURN_LOSSES = MyCrossEntropyLoss(reduction = REDUCTION).to(DEVICE)\n        print(\"Custiom LOSSES\")\n    else:\n        RETURN_LOSSES = BASE_LOSSES(reduction = REDUCTION).to(DEVICE)\n    return RETURN_LOSSES","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class build_default_model(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        if USE_BASE_TIMM_MODEL:\n            self.model = timm.create_model(BASE_MODEL, pretrained = LOAD_BASE_WEIGHTS)\n        else:\n            self.model = BASE_MODEL(pretrained = LOAD_BASE_WEIGHTS)\n\n        if not BASE_MODEL_TRAINABLE:\n            for param in self.model.parameters():\n                param.requires_grad = False\n\n        if INCLUDE_TOP:\n            # Alternatively, it can be generalized to nn.Linear(num_ftrs, CLASSES)\n            if USE_BASE_TIMM_MODEL:\n                n_features = self.model.classifier.in_features\n                self.model.classifier = nn.Linear(n_features, CLASSES)\n            else:\n                n_features = self.model.fc.in_features\n                self.model.fc = nn.Linear(n_features, CLASSES, bias = BIAS)\n        else:\n            if USE_BASE_TIMM_MODEL:\n                n_features = self.model.classifier.in_features\n                self.model.classifier = nn.Sequential(\n                    nn.Dropout(DROPOUT),\n                    nn.Linear(n_features, CLASSES, bias = BIAS)\n                )\n            else:\n                n_features = self.model.fc.in_features\n                self.model.fc = nn.Sequential(\n                    nn.Dropout(DROPOUT),\n                    nn.Linear(n_features, CLASSES, bias = BIAS)\n                )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class build_custiom_model(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. 定義回調函數方法 <a class=\"anchor\" id=\"6\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"def get_callbacks(optimizer):\n    if CALLBACKS_STEPLR:\n        # 等間隔調整學習率，調整倍數為gamma倍，調整間隔為step_size。間隔單位是step。需要注意的是，step通常是指epoch，不要弄成iteration了。\n        callbacks_scheduler = lr_scheduler.StepLR(optimizer, step_size = STEP_SIZE, gamma = GAMMA)\n    elif CALLBACKS_REDUCELRONPLATEAU:\n        # 當某指標不再變化（下降或升高），調整學習率，這是非常實用的學習率調整策略。例如，當驗證集的loss不再下降時，進行學習率調整；\n        # 或者監測驗證集的accuracy，當accuracy不再上升時，則調整學習率。\n        callbacks_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,  mode = MODE, factor = FACTOR, patience = PATIENCE, \n                                                   threshold = THRESHOLD, threshold_mode = THRESHOLD_MODE, cooldown = COOLDOWN, \n                                                   min_lr = MIN_LR, eps = SCHEDULER_EPS, verbose = SCHEDULER_VERBOSE)\n    elif CALLBACKS_COSINEANNEALINGWARMRESTAERS:\n        # 使用餘弦退火時間表設置每個參數組的學習率\n        callbacks_scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = T_0, T_mult = T_MULT, eta_min = ETA_MIN, \n                                                             verbose = SCHEDULER_VERBOSE)\n    else:\n        callbacks_scheduler = None \n    return callbacks_scheduler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. 製作資料集＆資料擴增&訓練模型 <a class=\"anchor\" id=\"7\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, df, one_hot_label = False, transforms = None):\n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        self.one_hot_label = one_hot_label\n        self.transforms = transforms\n        self.labels = self.df[LABEL_NAME].values\n        \n        if one_hot_label is True:\n            self.labels = np.eye(self.df[LABEL_NAME].max()+1)[self.labels]\n            #print(self.labels)\n        \n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, index: int):\n        label = self.labels[index]\n        image_name = self.df[IMAGE_NAME].values[index]\n        if IMAGE_NAME_HAVE_EXTENSION:\n            image_path = TRAIN_DATA_PATH + image_name\n        else:\n            image_path = TRAIN_DATA_PATH + image_name + IMAGE_NAME_EXTENSION\n        image = cv2.imread(image_path)\n        \n        if COLOR_CONVERT_RGB:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.transforms is not None:\n            image = self.transforms(image = image)['image']\n            \n        return image, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 確定是否將應用此增強。機率為 p = 1.0 意味著我們總是從上面應用轉換。\n# p = 0 將意味著將忽略轉換塊。\n# 0 < p < 1.0 等於每個擴增都具有以一定概率應用的選項。\n# OneOf 隨機選取一種增強擴增\n\ndef get_train_transforms():\n    return A.Compose([\n        A.Blur(blur_limit = BLUR_LIMIT, \n               p = P_BLUR), # 模糊\n        A.HorizontalFlip(p = P_HORIZONTALFLIP), # 水平翻轉\n        A.VerticalFlip(p = P_VERTICALFLIP), # 垂直翻轉\n        A.Flip(p = P_FLIP), # 水平和垂直翻轉\n        A.Resize(height = RESIZE_HEIGHT, \n                 width = RESIZE_WIDTH, \n                 p = P_RESIZE), # 縮放\n        A.RandomResizedCrop(height = RANDOMRESIZEDCROP_HEIGHT, \n                            width = RANDOMRESIZEDCROP_WIDTH, \n                            scale = RANDOMRESIZEDCROP_SCALE, \n                            p = P_RANDOMRESIZEDCROP), #隨機縮放剪裁\n        A.RandomRotate90(p = P_RANDOMROTATE90), # 隨機旋轉90度\n        A.ShiftScaleRotate(shift_limit = SHIFTSCALEROTATE_SHIFT_LIMIT, \n                           scale_limit = SHIFTSCALEROTATE_SCALE_LIMIT, \n                           rotate_limit = SHIFTSCALEROTATE_ROTATE_LIMIT, \n                           p = P_SHIFTSCALEROTATE), # 平移縮放旋轉\n        A.ElasticTransform(alpha = ELATICTRANSFORM_ALPHA, \n                           sigma = ELATICTRANSFORM_SIGMA, \n                           alpha_affine = ELATICTRANSFORM_ALPHA_AFFINE, \n                           p = P_ELATICTRANSFORM), # 彈性變換\n        A.GridDistortion(num_steps = GRIDDISTORTION_NUM_STEPS, \n                         p = P_GRIDDISTORTION), # 網格失真\n        A.RandomBrightnessContrast(brightness_limit = RANDOMBRIGHTNESSCONTRAST_BRIGHTNESS_LIMIT, \n                                   contrast_limit = RANDOMBRIGHTNESSCONTRAST_CONTRAST_LIMIT, \n                                   p = P_RANDOMBRIGHTNESSCONTRAST_CONTRAST), # 隨機亮度對比度\n        A.HueSaturationValue(hue_shift_limit = HUESATURATIONVALUE_HUE_SHIFT_LIMIT, \n                             sat_shift_limit = HUESATURATIONVALUE_SAT_SHIFT_LIMIT, \n                             val_shift_limit = HUESATURATIONVALUE_VAL_SHIFT_LIMIT, \n                             p = P_HUESATURATIONVALUE), # 隨機色調飽和度值\n        A.CLAHE(clip_limit = CLAHE_CLIP_LIMIT, \n                p = P_CLAHE), # 將對比度受限的自適應直方圖均衡化應用於輸入圖像\n        A.Cutout(num_holes = COARSEDROPOUT_NUM_HOLES, \n                        max_h_size = COARSEDROPOUT_MAX_H_SIZE, \n                        max_w_size = COARSEDROPOUT_MAX_W_SIZE, \n                        p = P_COARSEDROPOUT), # 隨機在圖像上生成黑色矩形\n        A.Normalize(\n             mean = NORMALIZE_MEAN, \n             std = NORMALIZE_STD, \n            max_pixel_value = NORMALIZE_MAX_PIXEL_VALUE, \n            p = P_NORMALIZE), # 正規化。\n        ToTensorV2(p = P_TOTENSORV2) # 歸一化\n    ], p = P_TRAIN_TRANSFORMS)\n\ndef get_val_transforms():\n    return A.Compose([\n        A.Resize(height = RESIZE_HEIGHT, \n                 width = RESIZE_WIDTH, \n                 p = P_RESIZE), # 縮放\n        A.Normalize(\n             mean = NORMALIZE_MEAN,\n             std = NORMALIZE_STD, \n            max_pixel_value = NORMALIZE_MAX_PIXEL_VALUE, \n            p = P_NORMALIZE), # 正規化。\n        ToTensorV2(p = P_TOTENSORV2) # 歸一化\n    ], p = P_VAL_TRANSFORMS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_dataloader(fold, df, train_index, valid_index):\n    \n    if LOAD_CSV:\n        if FOLD >1:\n            train_data = df.loc[train_index,:]\n            validation_data = df.loc[valid_index,:]\n        else:\n            X_train, X_val, Y_train, Y_val = train_test_split(train_csv[IMAGE_NAME], \n                                                              train_csv[LABEL_NAME], \n                                                              test_size = DATA_SPLIT, \n                                                              random_state = SEED)\n            train_data = pd.DataFrame(X_train)\n            train_data.columns = [IMAGE_NAME]\n            train_data[LABEL_NAME] = Y_train\n\n            validation_data = pd.DataFrame(X_val)\n            validation_data.columns = [IMAGE_NAME]\n            validation_data[LABEL_NAME] = Y_val\n        \n    train_dataset = MyDataset(train_data, transforms = get_train_transforms())\n    val_dataset = MyDataset(validation_data, transforms = get_val_transforms())\n    \n    # 紀錄訓練集跟驗證集大小\n    train_dataset_size = len(train_dataset)\n    val_dataset_size = len(val_dataset)\n    \n    #for metrics\n    dataset_sizes = { 'train': train_dataset_size, 'val': val_dataset_size}\n    print(dataset_sizes)\n    \n    train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE[fold], pin_memory = PIN_MEMORY, \n                                               shuffle = True, num_workers = NUM_WORKERS, drop_last = DROP_LAST)\n    \n    val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE[fold], pin_memory = PIN_MEMORY, \n                                               shuffle = False, num_workers = NUM_WORKERS)\n    \n    return train_loader, val_loader, train_dataset_size, val_dataset_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(epoch, model, scaler, train_loss, optimizer, train_loader, train_dataset_size, \n                    scheduler = None, scheduler_batch_update = False):\n    # 設定模型為訓練模式\n    model.train()\n\n    running_loss = 0.0\n    outputs_all = []\n    labels_all = []\n\n    # 遍歷enumaretad批處理\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader), \n                position = TQDM_POSITION, leave = TQDM_LEAVE)\n    for batch_idx, (inputs, labels) in pbar:      \n        # 提取輸入和標籤\n        inputs = inputs.to(DEVICE).float()\n        labels = labels.to(DEVICE).long()\n\n        # 前向過程(model + loss)開啟 autocast\n        with autocast():\n            outputs = model(inputs)\n            loss = train_loss(outputs, labels)\n            \n        # 歸一化損失以說明批次累積\n        loss = loss / ACCUM_ITER \n            \n        # Scales loss. 為了梯度放大\n        # 反向傳播在autocast上下文之外\n        scaler.scale(loss).backward()\n\n        # 權重更新\n        if ((batch_idx + 1) %  ACCUM_ITER == 0) or ((batch_idx + 1) == len(train_loader)):\n\n            # scaler.step() 首先把梯度的值unscale回來.\n            # 如果梯度的值不是 infs 或者 NaNs, 那麼調用optimizer.step()來更新權重,\n            # 否則，忽略step調用，從而保證權重不更新（不被破壞）\n            scaler.step(optimizer)\n            \n            # 準備著，看是否要增大scaler\n            scaler.update()\n            \n            # 零參數梯度\n            optimizer.zero_grad() \n            \n            if scheduler is not None and scheduler_batch_update:\n                scheduler.step()\n                \n        # statistics\n        running_loss += loss.item()*inputs.size(0)\n        outputs_all += [torch.argmax(outputs, 1).detach().cpu().numpy()]\n        labels_all += [labels.detach().cpu().numpy()]\n        \n    epoch_loss = running_loss / train_dataset_size\n    outputs_all = np.concatenate(outputs_all)\n    labels_all = np.concatenate(labels_all)\n    epoch_accuracy = (outputs_all == labels_all).mean()\n    print('Epoch {} Train Loss: {:.4f} Train Accuracy: {:.4f}'.format(epoch, epoch_loss, epoch_accuracy))\n                \n    if scheduler is not None and not scheduler_batch_update:\n        scheduler.step()\n        \ndef valid_one_epoch(epoch, model, val_loss, val_loader, val_dataset_size, \n                    scheduler = None, scheduler_loss_update = False):\n    # 設定模型為評估模式\n    model.eval()\n\n    running_loss = 0.0\n    outputs_all = []\n    labels_all = []\n    \n    # 遍歷enumaretad批處理\n    pbar = tqdm(enumerate(val_loader), total=len(val_loader), \n                position = TQDM_POSITION, leave = TQDM_LEAVE)\n    for batch_idx, (inputs, labels) in pbar:\n        # 提取輸入和標籤\n        inputs = inputs.to(DEVICE).float()\n        labels = labels.to(DEVICE).long()\n        \n        outputs = model(inputs)\n        loss = val_loss(outputs, labels)\n        \n        # statistics\n        running_loss += loss.item()*inputs.size(0)\n        outputs_all += [torch.argmax(outputs, 1).detach().cpu().numpy()]\n        labels_all += [labels.detach().cpu().numpy()]\n        \n    epoch_loss = running_loss / val_dataset_size\n    outputs_all = np.concatenate(outputs_all)\n    labels_all = np.concatenate(labels_all)\n    epoch_accuracy = (outputs_all == labels_all).mean()\n    print('Epoch {} Val Loss: {:.4f} Val Accuracy: {:.4f}'.format(epoch, epoch_loss, epoch_accuracy))\n    \n    if scheduler is not None and VAL_ENABLE_SCHEDULER:\n        if scheduler_loss_update:\n            scheduler.step(epoch_loss)\n        else:\n            scheduler.step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save the losses and metrics for further visualization\nlosses = {'train':[], 'val':[]}\nmetrics = {'train':[], 'val':[]}\n\n# 宣告為訓練後混淆矩陣預測用\nall_labels = []; all_pred = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_process(fold, skf, train_index, valid_index):\n    if skf:\n        print('FOLD %i - IMAGE SIZE %i WITH %s AND BATCH_SIZE %i'%(fold+1,IMAGE_SIZE[fold],MODEL_NAME.upper(),BATCH_SIZE[fold]))\n    else:\n        print('IMAGE SIZE %i WITH %s AND BATCH_SIZE %i'%(IMAGE_SIZE[fold],MODEL_NAME.upper(),BATCH_SIZE[fold]))\n        \n    if LOAD_CSV and skf:\n        # (String)訓練模型FOLD>1的儲存路徑\n        SAVE_MODEL_PATH = PROJECT_PATH+r'/models/'+MODEL_NAME+'_fold_%i.pt'%(fold+1)\n    else:\n        SAVE_MODEL_PATH = TRAIN_MODEL_PATH\n    \n    train_loader, val_loader, train_dataset_size, val_dataset_size = prepare_dataloader(fold, train_csv, train_index, valid_index)\n    \n    # 載入模型或權重\n    if LOAD_MODEL:\n        # load model\n        model = torch.load(LOAD_MODEL_PATH)\n    elif USE_BASE_MODEL:\n        # 創建model，默認是torch.FloatTensor\n        model = build_default_model()\n    else:\n        # ==== INIT CUSTIOM MODEL\n        model = build_custiom_model()\n        if LOAD_WEIGHTS:\n            # load model weights\n            model.load_state_dict(LOAD_WEIGHTS_PATH)\n\n    model.to(DEVICE)\n    optimizer = build_optimizers(model)\n    \n    # 在訓練最開始之前實例化一個GradScaler對象\n    scaler = GradScaler()\n    \n    scheduler = get_callbacks(optimizer) # 回調函式\n\n    train_loss = build_losses() # train loss\n    val_loss = build_losses() # val loss\n\n    if MODEL_PRINT:\n        # Print model's state_dict\n        print(\"Model's state_dict:\")\n        for param_tensor in model.state_dict():\n            print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n\n    if OPTIMIZER_PRINT:\n        # Print optimizer's state_dict\n        print(\"Optimizer's state_dict:\")\n        for var_name in optimizer.state_dict():\n            print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n               \n    for epoch in range(EPOCHS[fold]):\n        train_one_epoch(epoch, model, scaler, train_loss, optimizer, train_loader, train_dataset_size, scheduler = scheduler, \n                        scheduler_batch_update = SCHEDULER_BATCH_UPDATE)\n\n        with torch.no_grad():\n            valid_one_epoch(epoch, model, val_loss, val_loader, val_dataset_size, scheduler = scheduler, \n                            scheduler_loss_update = SCHEDULER_LOSS_UPDATE)\n\n#         torch.save(model.state_dict(),'{}_fold_{}_{}'.format(CFG['model_arch'], fold, epoch))\n            \n            \n            \n            \n            \n            \n            \n            \n    \n    \n#     val_acc = None\n#     val_loss = None\n#     val_auc = None\n#     early_stopping = 0\n    \n#     for epoch in range(EPOCHS):\n#         print('Epoch: {}/{}'.format(epoch, EPOCHS - 1))\n#         print('-' * 10)\n        \n#         # Each epoch has a training and validation phase\n#         for phase in ['train', 'val']:\n#             if phase == 'train':\n#                 model.train() # Set model to training mode\n#             else:\n#                 model.eval() # Set model to evaluate mode\n                \n#             running_loss = 0.0\n#             running_corrects = 0.0\n            \n#             # 宣告為計算每次迭代auc\n#             valid_preds, valid_targets = [], []\n                \n#             # Iterate over data.\n#             pbar = tqdm(loaders[phase], total = len(loaders[phase]), position = TQDM_POSITION, leave = TQDM_LEAVE)\n#             for idx, data in enumerate(pbar):\n#                 # get the inputs; data is a list of [inputs, labels]\n#                 inputs, labels = data[0].to(DEVICE), data[1].to(DEVICE).long()\n                \n#                 # zero the parameter gradients\n#                 optimizer.zero_grad()\n                \n#                 # forward\n#                 # track history if only in train\n#                 with torch.set_grad_enabled(phase=='train'):\n#                     outputs = model(inputs)\n#                     _, pred = torch.max(outputs, 1)\n#                     loss = criterion(outputs, labels)\n\n#                     # backward + optimize only if in training phase\n#                     if phase == 'train':\n#                         loss.backward()\n#                         optimizer.step()\n#                     else:\n#                         valid_preds.append(torch.softmax(outputs,1)[:,1].detach().cpu().numpy())\n#                         valid_targets.append(labels.detach().cpu().numpy())\n                   \n#                 # statistics\n#                 running_loss += loss.item()*inputs.size(0)\n#                 running_corrects += torch.sum(pred == labels.data)\n            \n#             epoch_loss = running_loss / dataset_sizes[phase]\n#             epoch_acc = running_corrects.double()/dataset_sizes[phase]\n#             losses[phase].append(epoch_loss)\n#             accuracies[phase].append(epoch_acc)\n            \n#             # 為了計算每次迭代auc\n#             valid_preds = np.concatenate(valid_preds)\n#             valid_targets = np.concatenate(valid_targets)\n#             epoch_auc =  roc_auc_score(valid_targets, valid_preds)\n            \n#             # 為了計算全部迭代混淆矩陣\n#             all_pred = np.concatenate(valid_preds)\n#             all_labels = np.concatenate(valid_targets)\n            \n# #             print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n            \n#             if phase == 'val':\n                \n#                 if CALLBACKS_CHECK_POINTER:\n#                     save_model = False\n#                     if SAVE_BEST_ONLY:\n#                         if epoch_acc > val_acc and MONITOR == \"val_acc\":\n#                             val_acc = epoch_acc\n#                             save_model = True\n#                         elif epoch_loss < val_loss and MONITOR == \"val_loss\":\n#                             val_loss = epoch_loss\n#                             save_model = True\n#                         elif epoch_auc > val_auc and MONITOR == \"val_auc\":\n#                             val_auc = epoch_auc\n#                             save_model = True\n#                     else:\n#                         if MONITOR == \"val_acc\":\n#                             val_acc = epoch_acc\n#                         elif MONITOR == \"val_loss\":\n#                             val_loss = epoch_loss\n#                         elif MONITOR == \"val_auc\":\n#                             val_auc = epoch_auc\n#                         save_model = True\n                        \n#                     if SAVE_WEIGHTS_ONLY and save_model:\n#                         torch.save(model.state_dict(), SAVE_MODEL_PATH)\n#                     elif not SAVE_WEIGHTS_ONLY and save_model:\n#                         torch.save(model, SAVE_MODEL_PATH)\n                \n# #         scheduler.step()\n    \n#     if SAVE_BEST_ONLY:\n#         if MONITOR == \"val_acc\":\n#             print('Best val Acc: {:4f}'.format(val_acc))\n#         elif MONITOR == \"val_loss\":\n#             print('Best val Loss: {:4f}'.format(val_loss))\n#         elif MONITOR == \"val_auc\":\n#             print('Best val AUC: {:4f}'.format(val_auc))\n            \n#     del train_loader, val_loader\n#     if LOAD_CSV and skf:\n#         del model\n#     torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train_process(fold, skf, train_index, valid_index):\n#     if skf:\n#         print('FOLD %i - IMAGE SIZE %i WITH %s AND BATCH_SIZE %i'%(fold+1,IMAGE_SIZE[fold],MODEL_NAME.upper(),BATCH_SIZE[fold]))\n#     else:\n#         print('IMAGE SIZE %i WITH %s AND BATCH_SIZE %i'%(IMAGE_SIZE[fold],MODEL_NAME.upper(),BATCH_SIZE[fold]))\n    \n#     loaders = prepare_dataloader(train_csv, train_index, valid_index)\n    \n#     val_acc = 0.0\n#     val_loss = 0.0\n#     val_auc = 0.0\n#     early_stopping = 0\n    \n#     if LOAD_CSV and skf:\n#         # (String)訓練模型FOLD>1的儲存路徑\n#         SAVE_MODEL_PATH = PROJECT_PATH+r'/models/'+MODEL_NAME+'_fold_%i.pt'%(fold+1)\n#     else:\n#         SAVE_MODEL_PATH = TRAIN_MODEL_PATH\n    \n#     since = time.time()\n#     for epoch in range(EPOCHS):  \n#         print('Epoch: {}/{}'.format(epoch, EPOCHS - 1))\n#         print('-' * 10)\n        \n#         # Each epoch has a training and validation phase\n#         for phase in ['train', 'val']:\n#             if phase == 'train':\n#                 model.train() # Set model to training mode\n#             else:\n#                 model.eval() # Set model to evaluate mode\n                \n#             running_loss = 0.0\n#             running_corrects = 0.0\n            \n#             # 宣告為計算每次迭代auc\n#             valid_preds, valid_targets = [], []\n                \n#             # Iterate over data.\n#             pbar = tqdm(loaders[phase], total = len(loaders[phase]), position = TQDM_POSITION, leave = TQDM_LEAVE)\n#             for idx, data in enumerate(pbar):\n#                 # get the inputs; data is a list of [inputs, labels]\n#                 inputs, labels = data[0].to(DEVICE), data[1].to(DEVICE).long()\n                \n#                 # zero the parameter gradients\n#                 optimizer.zero_grad()\n                \n#                 # forward\n#                 # track history if only in train\n#                 with torch.set_grad_enabled(phase=='train'):\n#                     outputs = model(inputs)\n#                     _, pred = torch.max(outp, 1)\n#                     loss = criterion(outputs, labels)\n\n#                     # backward + optimize only if in training phase\n#                     if phase == 'train':\n#                         loss.backward()\n#                         optimizer.step()\n#                     else:\n#                         valid_preds.append(torch.softmax(outputs,1)[:,1].detach().cpu().numpy())\n#                         valid_targets.append(labels.detach().cpu().numpy())\n                   \n#                 # statistics\n#                 running_loss += loss.item()*inputs.size(0)\n#                 running_corrects += torch.sum(pred == labels.data)\n            \n#             epoch_loss = running_loss / dataset_sizes[phase]\n#             epoch_acc = running_corrects.double()/dataset_sizes[phase]\n#             losses[phase].append(epoch_loss)\n#             accuracies[phase].append(epoch_acc)\n            \n#             # 為了計算每次迭代auc\n#             valid_preds = np.concatenate(valid_preds)\n#             valid_targets = np.concatenate(valid_targets)\n#             epoch_auc =  roc_auc_score(valid_targets, valid_preds)\n            \n#             # 為了計算全部迭代混淆矩陣\n#             all_pred = np.concatenate(valid_preds)\n#             all_labels = np.concatenate(valid_targets)\n            \n#             print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n            \n#             if phase == 'val':\n#                 print('Time: {}m {}s'.format((time.time()- since)//60, (time.time()- since)%60))\n                \n#                 if CALLBACKS_CHECK_POINTER:\n#                     save_model = False\n#                     if SAVE_BEST_ONLY:\n#                         if epoch_acc > val_acc and MONITOR == \"val_acc\":\n#                             val_acc = epoch_acc\n#                             save_model = True\n#                         elif epoch_loss < val_loss and MONITOR == \"val_loss\":\n#                             val_loss = epoch_loss\n#                             save_model = True\n#                         elif epoch_auc > val_auc and MONITOR == \"val_auc\":\n#                             val_auc = epoch_auc\n#                             save_model = True\n#                     else:\n#                         if MONITOR == \"val_acc\":\n#                             val_acc = epoch_acc\n#                         elif MONITOR == \"val_loss\":\n#                             val_loss = epoch_loss\n#                         elif MONITOR == \"val_auc\":\n#                             val_auc = epoch_auc\n#                         save_model = True\n                        \n#                     if SAVE_WEIGHTS_ONLY and save_model:\n#                         torch.save(model.state_dict(), SAVE_MODEL_PATH)\n#                     elif not SAVE_WEIGHTS_ONLY and save_model:\n#                         torch.save(model, SAVE_MODEL_PATH)\n                \n# #         scheduler.step()\n#     time_elapsed = time.time() - since\n#     print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    \n#     if SAVE_BEST_ONLY:\n#         if MONITOR == \"val_acc\":\n#             print('Best val Acc: {:4f}'.format(val_acc))\n#         elif MONITOR == \"val_loss\":\n#             print('Best val Loss: {:4f}'.format(val_loss))\n#         elif MONITOR == \"val_auc\":\n#             print('Best val AUC: {:4f}'.format(val_auc))\n            \n#     del loaders\n#     if LOAD_CSV and skf:\n#         del model\n#     torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    try:\n        since = time.time()\n        \n        if FOLD > 1:\n            for fold,(train_index, valid_index) in enumerate(SKF.split(np.arange(train_csv.shape[0]), train_csv[LABEL_NAME])):\n                train_process(fold = fold, skf = True, train_index = train_index, valid_index = valid_index)\n        else:\n            train_process(fold = 0, skf = False, train_index = None, valid_index = None)\n            \n        time_elapsed = time.time() - since\n        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))  \n    except Exception as exception:\n        print(exception)\n        raise","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. 混淆矩陣<a class=\"anchor\" id=\"8\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"markdown","source":"# 9. 待辦事項<a class=\"anchor\" id=\"9\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"markdown","source":"改訓練loss auc accuracy(train/val)\n\n存模型\n1. 回調函數(提早停止，tensroboard)\n2. 列印訓練過程\n3. 混淆矩陣&kappa\n4. focal loss\n5. 製作資料集(無csv訓練)\n6. 訓練(無csv訓練)","metadata":{}}]}