{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image_Classification_Pytorch (Training)\nhttps://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html","metadata":{}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# Table of Contents\n\n1. [套件安裝與載入](#1)\n1. [環境檢測與設定](#2)\n1. [開發參數設定](#3)\n1. [資料處理](#4)\n    -  [載入CSV檔](#4.1)\n    -  [檢查CSV檔缺失值](#4.2)\n1. [定義模型方法](#5)\n1. [定義回調函數方法](#6)\n1. [製作資料集＆資料擴增&回調函數&訓練模型](#7)\n1. [混淆矩陣 & Quadratic Weighted Kappa](#8)\n1. [待辦事項](#9)","metadata":{}},{"cell_type":"markdown","source":"# 1. 套件安裝與載入<a class=\"anchor\" id=\"1\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"!pip3 install git+https://github.com/rwightman/pytorch-image-models.git","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/rwightman/pytorch-image-models.git\n  Cloning https://github.com/rwightman/pytorch-image-models.git to /tmp/pip-req-build-yl0pkchq\n  Running command git clone -q https://github.com/rwightman/pytorch-image-models.git /tmp/pip-req-build-yl0pkchq\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm==0.4.8) (1.7.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm==0.4.8) (0.8.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.4.8) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.4.8) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.4.8) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.4.8) (1.19.5)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.4.8) (7.2.0)\nBuilding wheels for collected packages: timm\n  Building wheel for timm (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for timm: filename=timm-0.4.8-py3-none-any.whl size=338238 sha256=273257032a6ddab3f5467ac5f882af9d7ec1255f3ad8b072f9e494a02892cbf8\n  Stored in directory: /tmp/pip-ephem-wheel-cache-6s9pttfi/wheels/a0/ec/5f/289118b747739bb1e02e36cf3d7e759721e881c183653719dc\nSuccessfully built timm\nInstalling collected packages: timm\nSuccessfully installed timm-0.4.8\n","output_type":"stream"}]},{"cell_type":"code","source":"# 資料處理套件\nimport os\nimport cv2\nimport sys\nimport time\nimport timm\nimport copy\nimport random\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport albumentations as A\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import (roc_auc_score, confusion_matrix, accuracy_score, \n                             precision_score, recall_score, f1_score, classification_report, cohen_kappa_score)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 設定顯示中文字體\nfrom matplotlib.font_manager import FontProperties\nplt.rcParams['font.sans-serif'] = ['Microsoft JhengHei'] # 用來正常顯示中文標籤\nplt.rcParams['font.family'] = 'AR PL UMing CN'\nplt.rcParams['axes.unicode_minus'] = False # 用來正常顯示負號","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# pytorch深度學習模組套件\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torch.nn.functional as F\nimport torchvision\n\nfrom torch.optim import lr_scheduler\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import models","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# 2. 環境檢測與設定<a class=\"anchor\" id=\"2\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDEVICE","metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"code","source":"'''執行環境參數設定'''\n\n# (Boolean)是否為本機\nLOCAL = False\n\n# (Boolean)是否為 Colab\nCOLAB = False\n\n\n'''檔案路徑參數設定'''\n\n# (String)Root路徑\nif LOCAL:\n    PATH = r'../'\nelif COLAB:\n    PATH = r'/content/drive/My Drive/Colab Notebooks/'\nelse:\n    PATH = r'../input/'\n    OUTPUT_PATH = r'/kaggle/working/'\n    \n# (String)資料根路徑\nDATA_ROOT_PATH = PATH+r'aptos2019-blindness-detection/' \n\n# (String)訓練資料路徑\nTRAIN_DATA_PATH = DATA_ROOT_PATH+r'train_images/'\n\n# (String)訓練CSV路徑，如為None則不讀CSV檔\nTRAIN_CSV_PATH = DATA_ROOT_PATH+r'train.csv'\n\n# (String)專案名稱\nPROJECT_NAME = 'Blindness Detection'\n\n# (String)專案檔案儲存路徑\nif LOCAL or COLAB:\n    OUTPUT_PATH = PATH\nPROJECT_PATH = OUTPUT_PATH+PROJECT_NAME+'/'+PROJECT_NAME+' '+datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n\n# (String)權重名稱(使用哪個權重)\nWEIGHTS_NAME = 'resnet18'\n\n# (String)模型名稱(使用哪個模型)\nMODEL_NAME = 'resnet18'\n\n# (String)讀取預訓練權重的儲存路徑 \nLOAD_WEIGHTS_PATH = PROJECT_PATH+r'/models/backup/'+WEIGHTS_NAME+'.pth'\n\n# (String)讀取預訓練模型的儲存路徑 \nLOAD_MODEL_PATH = PROJECT_PATH+r'/models/backup/'+MODEL_NAME+'.pth'\n\n# (String)訓練模型的儲存路徑\nTRAIN_MODEL_PATH = PROJECT_PATH+r'/models/'+MODEL_NAME+'.pth'","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if not LOCAL and COLAB:\n    from google.colab import drive\n    drive.mount('/content/drive')","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"if DEVICE != \"CPU\":\n    !nvidia-smi","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/bin/bash: nvidia-smi: command not found\n","output_type":"stream"}]},{"cell_type":"code","source":"if os.path.isfile(TRAIN_CSV_PATH):\n    LOAD_CSV = True\nelse:\n    LOAD_CSV = False","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"if not os.path.isdir(PROJECT_PATH+r'/models/'):\n    os.makedirs(PROJECT_PATH+r'/models/')","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# 3. 開發參數設定<a class=\"anchor\" id=\"3\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"'''客製參數設定'''\n\n\n'''資料參數設定'''\n\n# (Int)分類數量\nCLASSES = 5\n\n# (Int)有CSV檔該參數才有用，1則為不做交叉驗證\nFOLD = 1\n    \n# (Int)圖片尺寸\nIMAGE_SIZE = [224]*FOLD\n\n# (String)圖片副檔名\nIMAGE_NAME_EXTENSION = '.png'\n\n# (String)CSV圖片檔名欄位\nIMAGE_NAME = 'id_code'\n\n# (String)CSV標籤欄位\nLABEL_NAME = 'diagnosis'\n\n# (String)CSV標籤欄位類型\nLABEL_NAME_TYPE = 'string'\n\n# (Boolean)CSV圖片檔名欄位是否包含副檔名\nIMAGE_NAME_HAVE_EXTENSION = False\n\n# (Boolean)是否轉DataSet時，讓標籤做獨熱編碼\nONE_HOT_LABEL = False\n\n#  (Boolean)圖像轉為RGB\nCOLOR_CONVERT_RGB = True\n\n# (Int)不同的種子會產生不同的Random或分層K-FOLD分裂, 42則是預設固定種子\nSEED = 42\n\nif FOLD == 1:\n    # (Float)驗證集佔訓練集的比率，FOLD>1則不啟用\n    DATA_SPLIT = 0.2\nelse:\n    # (String)切分訓練集跟驗證集方式\n    SKF = StratifiedKFold(n_splits=FOLD,shuffle=True,random_state=SEED)\n\n# (Boolean)是否資料轉Tensor時啟動鎖頁內存(GPU內存)，而不鎖頁內存就是會使用到硬碟虛擬內存\nPIN_MEMORY = False\n\n# (Int)要用於數據加載的子進程數。0表示將在主進程中加載數據。（默認值：0）\nNUM_WORKERS = 0\n\n# (Boolean)批次處理在大小不合適的情況下，是否刪除最後一個不完整的批次\nDROP_LAST = False\n    \n# (Boolean)如為True每次返回的卷積算法將是確定的，即默認算法\nCUDNN_DETERMINISTIC = True\n\n# (Boolean)PyTorch 中對模型裡的卷積層進行預先的優化，也就是在每一個卷積層中測試 cuDNN 提供的所有卷積實現算法，\n# 然後選擇最快的那個。這樣在模型啟動的時候，只要額外多花一點點預處理時間，就可以較大幅度地減少訓練時間\nCUDNN_BENCHMARK = True\n\n\n'''資料擴增參數設定\n\n資料擴增範例\nhttps://zh-hant.hotbak.net/key/albumentation%E6%95%B8%E6%93%9A%E5%A2%9E%E5%BC%B7CSDN.html\n\n資料擴增教學\nhttps://zhuanlan.zhihu.com/p/107399127\n\n資料擴增Doc\nhttps://vfdev-5-albumentations.readthedocs.io/en/docs_pytorch_fix/api/augmentations.html\n'''\n\n# (Float)訓練集資料擴增的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_TRAIN_TRANSFORMS = 1.0\n\n# (Float)驗證集資料擴增的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_VAL_TRANSFORMS = 1.0\n\n# 以下資料擴增為訓練集使用=============================================\n\n# (Float)模糊的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_BLUR = 0\n\n# (Int)模糊的上限\nBLUR_LIMIT = 3\n\n# (Float)水平翻轉的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_HORIZONTALFLIP = 0\n\n# (Float)垂直翻轉的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_VERTICALFLIP = 0\n\n# (Float)水平和垂直翻轉的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_FLIP = 0\n\n# (Float)隨機旋轉90度的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_RANDOMROTATE90 = 0\n\n# (Float)平移縮放旋轉的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_SHIFTSCALEROTATE = 0\n\n# (Float)平移縮放旋轉的平移上限\nSHIFTSCALEROTATE_SHIFT_LIMIT = 0.0625\n\n# (Float)平移縮放旋轉的縮放上限\nSHIFTSCALEROTATE_SCALE_LIMIT = 0.1\n\n# (Float)平移縮放旋轉的旋轉上限\nSHIFTSCALEROTATE_ROTATE_LIMIT = 45\n\n# (Float)彈性變換的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_ELATICTRANSFORM = 0\n\n# (Float)彈性變換的alpha高斯過濾參數\nELATICTRANSFORM_ALPHA = 1\n\n# (Float)彈性變換的sigma高斯過濾參數\nELATICTRANSFORM_SIGMA = 50\n\n# (Float)彈性變換的alpha_affine，範圍為（-alpha_affine，alpha_affine）\nELATICTRANSFORM_ALPHA_AFFINE = 50\n\n# (Float)網格失真的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_GRIDDISTORTION = 0\n\n# (Int)網格失真的每一條邊上網格單元數量\nGRIDDISTORTION_NUM_STEPS = 5\n\n# (Float)隨機亮度對比度的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_RANDOMBRIGHTNESSCONTRAST_CONTRAST = 0\n\n# (Float)隨機亮度的上限\nRANDOMBRIGHTNESSCONTRAST_BRIGHTNESS_LIMIT = 0.2\n\n# (Float)隨機對比度的上限\nRANDOMBRIGHTNESSCONTRAST_CONTRAST_LIMIT = 0.2\n\n# (Float)隨機色調飽和度的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_HUESATURATIONVALUE = 0\n\n# (Float)隨機色調飽和度的色調上限\nHUESATURATIONVALUE_HUE_SHIFT_LIMIT = 20\n\n# (Float)隨機色調飽和度的飽和度上限\nHUESATURATIONVALUE_SAT_SHIFT_LIMIT = 30\n\n# (Float)隨機色調飽和度的值上限\nHUESATURATIONVALUE_VAL_SHIFT_LIMIT = 20\n\n# (Float)對比度受限自適應直方圖均衡的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_CLAHE = 0\n\n# (Float)對比度受限自適應直方圖均衡的對比度上限\nCLAHE_CLIP_LIMIT = 4.0\n\n# (Float)隨機在圖像上生成黑色矩形的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_COARSEDROPOUT = 0\n\n# (Int)隨機在圖像上生成黑色矩形的數量\nCOARSEDROPOUT_NUM_HOLES = 8\n\n# (Int)隨機在圖像上生成黑色矩形的最大高度\nCOARSEDROPOUT_MAX_H_SIZE = 8\n\n# (Int)隨機在圖像上生成黑色矩形的最大寬度\nCOARSEDROPOUT_MAX_W_SIZE = 8\n\n# (Float)隨機縮放剪裁的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_RANDOMRESIZEDCROP = 0\n\n# (Float Tuple)隨機縮放剪裁之前的圖像比例縮放\nRANDOMRESIZEDCROP_SCALE = (0.08, 1.0)\n\n# (Int)隨機縮放剪裁之前的圖像高度\nRANDOMRESIZEDCROP_HEIGHT = IMAGE_SIZE[0]\n\n# (Int)隨機縮放剪裁之前的圖像寬度\nRANDOMRESIZEDCROP_WIDTH = IMAGE_SIZE[0]\n\n# 以上資料擴增為訓練集使用=============================================\n\n# 以下資料擴增為訓練集和驗證集共用======================================\n\n# (Float)縮放的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_RESIZE = 1.0\n\n# (Int)縮放後的圖片高度\nRESIZE_HEIGHT = IMAGE_SIZE[0]\n\n# (Int)縮放後的圖片寬度\nRESIZE_WIDTH = IMAGE_SIZE[0]\n\n# (Float)正規化的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\nP_NORMALIZE = 1.0\n\n# (List)正規化的平均值(Imagenet的參考平均值[0.485, 0.456, 0.406])\nNORMALIZE_MEAN = [0.485, 0.456, 0.406]\n\n# (List)正規化的標準差(Imagenet的參考標準差[0.229, 0.224, 0.225])\nNORMALIZE_STD = [0.229, 0.224, 0.225]\n\n# (Float)正規化的PIXEL最大值(Imagenet的參考PIXEL最大值255.0)\nNORMALIZE_MAX_PIXEL_VALUE = 255.0\n\n# (Float)歸一化的啟用(0:不啟用,1.0:一律啟用,小數點:機率啟用)\n# ToTensorV2()將[0, 255]的PIL.Image或[H, W, C]的numpy.ndarray數據，\n# 轉換為形狀[C, H, W]的torch.FloadTensor，並歸一化到[0, 1.0]。\nP_TOTENSORV2 = 1.0\n\n# 以上資料擴增為訓練集和驗證集共用======================================\n\n\n''''模型參數設定'''\n\n# (Boolean)使用基礎模型，如為False則須客制另外撰寫\nUSE_BASE_MODEL = True\n\nif USE_BASE_MODEL:\n    # (Boolean)使用基礎timm模型，如為False則使用基礎Pytorch模型\n    USE_BASE_TIMM_MODEL = False\n    if USE_BASE_TIMM_MODEL:\n        # (Model)建立timm模型\n        BASE_MODEL = \"tf_efficientnet_b4_ns\"\n    else:\n        # (Model)建立Pytorch模型\n        BASE_MODEL = models.resnet18\n    \n# (Boolean)是否使用基礎模型權重\nLOAD_BASE_WEIGHTS = True\n\n# (Boolean)基礎模型是否包含完全連接網路頂部的網路層\nINCLUDE_TOP = False\n\n# (Boolean)基礎模型是否可訓練權重(不包括頂部網路層)\nBASE_MODEL_TRAINABLE = False\n\n# (Boolean)是否已有客製模型，僅載入權重\nLOAD_WEIGHTS = False\n\n# (Boolean)是否載入完整客製(模型+權重)\nLOAD_MODEL = False\n\n# (Float)Dropout比率 0.5\nDROPOUT = 0.5\n\n# (Boolean)Bias偏移量\nBIAS = True\n\n# (Boolean)是否印出完整模型\nMODEL_PRINT = False\n\n\n''''回調函數參數設定\n\n學習率遞減\nhttps://zhuanlan.zhihu.com/p/69411064\n\n回調函數Doc\nhttps://pytorch.org/docs/stable/optim.html\n\n模型儲存\nhttps://pytorch.org/tutorials/beginner/saving_loading_models.html\n\n'''\n\n# (Boolean)回調函數 ModelCheckpoint 是否啟用\nCALLBACKS_CHECK_POINTER = True\n\n# (Boolean)回調函數 EarlyStopping 是否啟用\nCALLBACKS_EARLY_STOPPING = False\n\n# (Boolean)回調函數 StepLR 是否啟用\nCALLBACKS_STEPLR = True\n\n# (Boolean)回調函數 ReduceLROnPlateau 是否啟用\nCALLBACKS_REDUCELRONPLATEAU = False\n\n# (Boolean)回調函數 CosineAnnealingWarmRestarts 是否啟用\nCALLBACKS_COSINEANNEALINGWARMRESTAERS = False\n\n# (String)回調函數監控數值(val_auc 僅限雙分類) val_acc/val_loss/val_auc\nMONITOR = 'val_loss'\n\n# (Boolean)回調函數 ModelCheckpoint 是否只儲存最佳模型 False\nSAVE_BEST_ONLY = True\n\n# (Boolean)回調函數 ModelCheckpoint 是否只儲存權重 True\nSAVE_WEIGHTS_ONLY = True\n\n# (Int)回調函數 EarlyStopping 沒有改善的時期數，之後訓練將停止 10\nPATIENCE_ELS = 10\n\n# (Int)學習率衰減的時間段\nSTEP_SIZE = 15\n\n# (Float)學習率衰減的乘數 0.1\nGAMMA = 0.1\n\n# (String)最小，最大之一 在最小模式下，當監視的數量停止減少時，lr將減小； 在最大模式下，當監視的數量停止增加時，它將減少。 min\nMODE = \"min\"\n\n# (Float)學習率降低的因數。new_lr = lr *因子 0.1\nFACTOR = 0.1\n\n# (Int)沒有改善的時期數，此後學習率將降低。例如，如果 耐心= 2，那麼我們將忽略前兩個時期而沒有任何改善，\n# 並且如果損失仍然沒有改善，則只會在第三個時期之後降低LR。 10\nPATIENCE = 10 \n\n# (Float)用於測量新的最佳閾值，僅關注重大變化。 1e-4\nTHRESHOLD = 1e-4 \n\n# (String)rel，abs之一。在rel模式下，“ max”模式下的dynamic_threshold = best *（1 +閾值），在min模式下，\n# dynamic_threshold = best *（1-threshold）。在絕對模式下，dynamic_threshold =最佳+ 最大模式下的閾值或最佳-最小模式下的閾值。 rel\nTHRESHOLD_MODE = \"rel\"\n\n# (Int)減少lr後恢復正常運行之前要等待的時期數。 0 \nCOOLDOWN = 0\n\n# (Float/List)標量或標量列表。所有參數組或每個組的學習率的下限。 0 \nMIN_LR = 0\n\n# (Float)應用於lr的最小衰減。如果新舊lr之間的差異小於eps，則忽略該更新。 1e-8\nSCHEDULER_EPS = 1e-8\n\n# (Int)第一次重啟的迭代次數。\nT_0 = 15\n\n# (Int)重新啟動後，因素增加。 1 \nT_MULT = 1\n\n# (Int)最低學習率。 0\nETA_MIN = 0\n\n# (Boolean)每次更新，學習率回調函式都會向輸出印出一條消息。 False\nSCHEDULER_VERBOSE = False\n\n# (Boolean)訓練集每批就更新學習率回調函式，否則每時代就更新。 False\nSCHEDULER_BATCH_UPDATE = False\n\n# (Boolean)驗證集學習率回調函式是否啟用。 False\nVAL_ENABLE_SCHEDULER = False\n\n# (Boolean)驗證集通過計算LOSS就更新學習率回調函式，否則不計算就更新。 False\nSCHEDULER_LOSS_UPDATE = False\n\n\n''''編譯參數設定\n\n編譯參數Doc\nhttps://pytorch.org/docs/stable/optim.html\n\n'''\n\n# (String)優化器指定(SGD/Adam/Adamax/RMSprop/Adagrad)，None為客制，須另外撰寫\nOPTIMIZERS_TYPE = \"Adam\"\n\n# (Float)優化器學習率 1e-3/1e-1\nLEARNING_RATE = 1e-3\n\n# (Float)學習速率衰減 0\nLR_DECAY = 0\n\n# (Float)優化器權重衰減 5e-5/5e-4\nWEIGHT_DECAY = 5e-5\n\n# (Float)加速優化器在相關方向上前進，並抑制震盪 0.9\nMOMENTUM = None\n\n# (Tuple Float)用於計算梯度及其平方的移動平均值的係數 (0.9, 0.999)\nBETAS = (0.9, 0.999)\n\n# (Float)分母中添加的項，以提高數值穩定性 1e-8\nEPS = 1e-8\n\n# (Float)平滑常數 0.99\nALPHA = 0.99\n\n# (Boolean)計算居中RMSProp，則通過估計其方差來對梯度進行歸一化 True\nCENTERED = True\n\n# (Float)阻尼動量 0\nDAMPENING = 0\n\n# (Boolean)啟用Nesterov動量 False\nNESTEROV = False\n\n# (String)損失函數，None為客制，須另外撰寫\nBASE_LOSSES = nn.CrossEntropyLoss\n\nif BASE_LOSSES is None:\n    # (Boolean)使用客制Focal loss損失函數\n    USE_FOCAL_LOSSES = True\n    if USE_FOCAL_LOSSES:\n        # (Float)對第一類影響進行減弱(目標檢測任務中,第一類為背景類) 0.25\n        # (Int List)分別對每一類施加不同的權重 [1,2,3,1,2]\n        FOCAL_LOSSES_ALPHA = 0.25\n        \n        # Focal loss的Gamma參數\n        FOCAL_LOSSES_GAMMA = 2\n\n# (String)指定還原成適用於輸出，預設mean\nREDUCTION = \"mean\"\n\n# (String )評價指標的圖表顯示 accuracy/auc\nPLOT_METRICS = 'accuracy'\n\n# (Boolean)是否印出完整編譯器\nOPTIMIZER_PRINT = False\n\n\n''''訓練參數設定'''\n\n# (Int List)每批訓練的尺寸\nBATCH_SIZE = [512]*FOLD\n\n# (Int)訓練做幾次時代\nEPOCHS = [1]*FOLD\n\n# (Int)指定列印進度條的位置（從0開始）\nTQDM_POSITION = 0\n\n# (Boolean)保留迭代結束時進度條的所有痕跡。如果是None，只會在position是0時離開\nTQDM_LEAVE = True\n\n# https://kozodoi.me/python/deep%20learning/pytorch/tutorial/2021/02/19/gradient-accumulation.html\n# (Int)假設您要在一批中使用32張圖像，但是一旦超出8張，硬件就會崩潰\n# 在這種情況下，您可以使用8張圖像的批次並每4批次更新一次權重，因此設置 4\nACCUM_ITER = 1\n\n# (Boolean)是否計算每個Epoch Step指標\nCALCULATE_EPOCH_STEP = True\n\n# (Boolean)是否計算每個Epoch Average指標\nCALCULATE_EPOCH_AVERAGE = True\n\n# (Boolean)需先開啟計算，是:畫出Epoch Average指標計算，否:畫出Epoch Step指標計算\nPLOT_EPOCH_AVERAGE = False\n\n# (Boolean)需先開啟計算，是:Tensorboard每個Epoch Average指標計算，否:Tensorboard每個Epoch Step指標計算\nTENSORBOARD_EPOCH_AVERAGE = False\n\n\n''''圖表參數設定'''\n\n# (Float)全部SNS圖表的字形縮放\nALL_SNS_FONT_SCALE = 1.0\n\n# (Int)CSV缺失值圖表寬度\nCSV_COUNTPLOT_FIGSIZE_W = 10\n\n# (Int)CSV缺失值圖表高度\nCSV_COUNTPLOT_FIGSIZE_H = 10\n\n# (Int)CSV缺失值圖表標題字型大小\nCSV_COUNTPLOT_TITLE_FONTSIZE = 20\n\n# (Int)CSV缺失值圖表X軸標題字型大小\nCSV_COUNTPLOT_XLABEL_FONTSIZE = 15\n\n# (Int)CSV缺失值圖表Y軸標題字型大小\nCSV_COUNTPLOT_YLABEL_FONTSIZE = 15\n\n# (Int)訓練歷程圖表寬度\nTRAINING_CURVES_FIGSIZE_W = 20\n\n# (Int)訓練歷程圖表高度\nTRAINING_CURVES_FIGSIZE_H = 10\n\n# (Int)訓練歷程圖表SCATTER的標記點大小 \nTRAINING_CURVES_SCATTER_SCALAR = 200\n\n# (Float)訓練歷程圖表SCATTER的指標文字離標記點X距離係數\nTRAINING_CURVES_SCATTER_METRICS_TEXT_XSCALAR = 0.03\n\n# (Float)訓練歷程圖表SCATTER的指標文字標記點Y距離係數\nTRAINING_CURVES_SCATTER_METRICS_TEXT_YSCALAR = 0.13\n\n# (Float)訓練歷程圖表SCATTER的損失文字標記點X距離係數\nTRAINING_CURVES_SCATTER_LOSS_TEXT_XSCALAR = 0.03\n\n# (Float)訓練歷程圖表SCATTER的損失文字標記點Y距離係數\nTRAINING_CURVES_SCATTER_LOSS_TEXT_YSCALAR = 0.05\n\n# (Int)訓練歷程圖表SCATTER的文字大小\nTRAINING_CURVES_SCATTER_TEXTSIZE = 15\n\n# (Int)訓練歷程圖表X軸標題字型大小\nTRAINING_CURVES_XLABEL_FONTSIZE = 15\n\n# (Int)訓練歷程圖表Y軸標題字型大小\nTRAINING_CURVES_YLABEL_FONTSIZE = 15\n\n# (Int)訓練歷程圖表標題字型大小\nTRAINING_CURVES_TITLE_FONTSIZE = 20\n\n# (Float)訓練歷程圖表格線粗度\nTRAINING_CURVES_GRID_ALPHA = 0\n\n# (Int)混淆矩陣圖表寬度\nCONFUSION_MATRIX_FIGSIZE_W = 10\n\n# (Int)混淆矩陣圖表高度\nCONFUSION_MATRIX_FIGSIZE_H = 10\n\n# (Int)混淆矩陣圖表內容字型大小\nCONFUSION_MATRIX_HEATMAP_FONTSIZE = 15\n\n# (Int)混淆矩陣圖表標題字型大小\nCONFUSION_MATRIX_TITLE_FONTSIZE = 20\n\n# (Int)混淆矩陣圖表X軸標題字型大小\nCONFUSION_MATRIX_XLABEL_FONTSIZE = 15\n\n# (Int)混淆矩陣圖表Y軸標題字型大小\nCONFUSION_MATRIX_YLABEL_FONTSIZE = 15\n\n# (String)預設'binary'：僅在目標為二進制，兩分類時適用。\n#'micro'：通過計算總的真陽性，假陰性和假陽性來全局計算指標，多分類時適用。\n#'macro'：計算每個標籤的指標，並找到其未加權平均值。這沒有考慮標籤不平衡，多分類時適用。\nAVERAGE = 'macro'","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = CUDNN_DETERMINISTIC\n    torch.backends.cudnn.benchmark = CUDNN_BENCHMARK\n\nseed_everything(SEED)","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# 設置sns圖表縮放係數\nsns.set(font_scale = ALL_SNS_FONT_SCALE)","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# 4. 資料處理<a class=\"anchor\" id=\"4\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"markdown","source":"## 4.1 載入CSV檔 <a class=\"anchor\" id=\"4.1\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"if LOAD_CSV:\n    print('Reading data...')\n\n    # 讀取訓練資料集CSV檔\n    train_csv = pd.read_csv(TRAIN_CSV_PATH,encoding=\"utf8\")\n\n    print('Reading data completed')","metadata":{"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Reading data...\nReading data completed\n","output_type":"stream"}]},{"cell_type":"code","source":"if LOAD_CSV:\n    # 顯示訓練資料集CSV檔\n    print(train_csv.head())","metadata":{"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"        id_code  diagnosis\n0  000c1434d8d7          2\n1  001639a390f0          4\n2  0024cdab0c1e          1\n3  002c21358ce6          0\n4  005b95c28852          0\n","output_type":"stream"}]},{"cell_type":"code","source":"if LOAD_CSV:\n    print(\"Shape of train_data :\", train_csv.shape)","metadata":{"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Shape of train_data : (3662, 2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 4.2 檢查CSV檔缺失值 <a class=\"anchor\" id=\"4.2\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"# 缺失值比率\nif LOAD_CSV:\n    total = train_csv.isnull().sum().sort_values(ascending = False)\n    percent = (train_csv.isnull().sum()/train_csv.isnull().count()*100).sort_values(ascending = False)\n    missing_train_csv  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    print(missing_train_csv.head())","metadata":{"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"           Total  Percent\nid_code        0      0.0\ndiagnosis      0      0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"if LOAD_CSV:\n    print(train_csv[LABEL_NAME].value_counts())\n    f,ax = plt.subplots(figsize=(CSV_COUNTPLOT_FIGSIZE_W, CSV_COUNTPLOT_FIGSIZE_H))\n    sns.countplot(train_csv[LABEL_NAME], hue = train_csv[LABEL_NAME],ax = ax)\n    plt.title(\"LABEL COUNT\", fontsize=CSV_COUNTPLOT_TITLE_FONTSIZE)\n    plt.xlabel(LABEL_NAME.upper(), fontsize=CSV_COUNTPLOT_XLABEL_FONTSIZE)\n    plt.ylabel(\"COUNT\", fontsize=CSV_COUNTPLOT_YLABEL_FONTSIZE)\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"0    1805\n2     999\n1     370\n4     295\n3     193\nName: diagnosis, dtype: int64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x720 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# 5. 定義模型方法<a class=\"anchor\" id=\"5\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"def build_optimizers(model):\n    if OPTIMIZERS_TYPE == None:\n        print(\"Custiom OPTIMIZERS\")\n    elif OPTIMIZERS_TYPE == \"SGD\":\n        RETURN_OPTIMIZERS = optim.SGD(model.parameters(), \n                                       lr = LEARNING_RATE, momentum = MOMENTUM, \n                                      dampening = DAMPENING, weight_decay = WEIGHT_DECAY, \n                                      nesterov = NESTEROV)\n    elif OPTIMIZERS_TYPE == \"Adam\":\n        RETURN_OPTIMIZERS = optim.Adam(model.parameters(), \n                                       lr = LEARNING_RATE, betas = BETAS, eps = EPS, \n                                       weight_decay = WEIGHT_DECAY)\n    elif OPTIMIZERS_TYPE == \"Adamax\":\n        RETURN_OPTIMIZERS = optim.Adamax(model.parameters(), \n                                         lr = LEARNING_RATE, betas = BETAS, eps = EPS, \n                                         weight_decay = WEIGHT_DECAY)\n    elif OPTIMIZERS_TYPE == \"RMSprop\":\n        RETURN_OPTIMIZERS = optim.RMSprop(model.parameters(), \n                                          lr = LEARNING_RATE, alpha = ALPHA, eps = EPS, \n                                          weight_decay = WEIGHT_DECAY, momentum = MOMENTUM, \n                                          centered = CENTERED)\n    elif OPTIMIZERS_TYPE == \"Adagrad\":\n        RETURN_OPTIMIZERS = optim.Adagrad(model.parameters(), \n                                          lr = LEARNING_RATE, lr_decay = LR_DECAY, \n                                          weight_decay = WEIGHT_DECAY)\n    return RETURN_OPTIMIZERS","metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class focal_loss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2, num_classes = 3, size_average=True):\n        \"\"\"\n        focal_loss損失函數, -α(1-yi)**γ *ce_loss(xi,yi)\n        步驟詳細的實現了 focal_loss損失函數.\n        :param alpha: 阿爾法α,類別權重. 當α是列表時,為各類別權重,當α為常數時,類別權重為[α, 1-α, 1-α, ....],常用於目標檢測算法中抑制背景類, retainnet中設置為0.25\n        :param gamma: 伽馬γ,難易樣本調節參數. retainnet中設置為2\n        :param num_classes: 類別數量\n        :param size_average: 損失計算方式,默認取均值\n        \"\"\"\n\n        super(focal_loss,self).__init__()\n        self.size_average = size_average\n        if isinstance(alpha,list):\n            assert len(alpha)==num_classes # α可以以list方式輸入,size:[num_classes] 用於對不同類別精細地賦予權重\n            print(\"Focal_loss alpha = {}, 將對每一類權重進行精細化賦值\".format(alpha))\n            self.alpha = torch.Tensor(alpha)\n        else:\n            assert alpha<1 #如果α為一個常數,則降低第一類的影響,在目標檢測中為第一類\n            print(\" --- Focal_loss alpha = {} ,將對背景類進行衰減,請在目標檢測任務中使用 --- \".format(alpha))\n            self.alpha = torch.zeros(num_classes)\n            self.alpha[0] += alpha\n            self.alpha[1:] += (1-alpha) # α 最終為 [ α, 1-α, 1-α, 1-α, 1-α, ...] size:[num_classes]\n        self.gamma = gamma\n\n    def forward(self, preds, labels):\n        \"\"\"\n        focal_loss損失計算\n        :param preds: 預測類別. size:[B,N,C] or [B,C] 分別對應與檢測與分類任務, B 批次, N檢測框數, C類別數\n        :param labels: 實際類別. size:[B,N] or [B]\n        :return:\n        \"\"\"\n        # assert preds.dim()==2 and labels.dim()==1\n        preds = preds.view(-1,preds.size(-1))\n        self.alpha = self.alpha.to(preds.device)\n        preds_softmax = F.softmax(preds, dim=1) # 這裡並沒有直接使用log_softmax, 因為後面會用到softmax的結果(當然你也可以使用log_softmax,然後進行exp操作)\n        preds_logsoft = torch.log(preds_softmax)\n        preds_softmax = preds_softmax.gather(1,labels.view(-1,1)) # 這部分實現nll_loss ( crossempty = log_softmax + nll )\n        preds_logsoft = preds_logsoft.gather(1,labels.view(-1,1))\n        self.alpha = self.alpha.gather(0,labels.view(-1))\n        loss = -torch.mul(torch.pow((1-preds_softmax), self.gamma), preds_logsoft) # torch.pow((1-preds_softmax), self.gamma) 為focal loss中(1-pt)** γ\n        loss = torch.mul(self.alpha, loss.t())\n        if self.size_average:\n            loss = loss.mean()\n        else:\n            loss = loss.sum()\n        return loss\n\n# reference: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/173733\nclass MyCrossEntropyLoss(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean'):\n        super().__init__(weight=weight, reduction=reduction)\n        self.weight = weight\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        lsm = F.log_softmax(inputs, -1)\n\n        if self.weight is not None:\n            lsm = lsm * self.weight.unsqueeze(0)\n\n        loss = -(targets * lsm).sum(-1)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n    \ndef build_losses():\n    if BASE_LOSSES == None:\n        if USE_FOCAL_LOSSES:\n            RETURN_LOSSES = focal_loss(alpha = FOCAL_LOSSES_ALPHA, gamma = FOCAL_LOSSES_GAMMA, num_classes = CLASSES).to(DEVICE)\n        else:\n            RETURN_LOSSES = MyCrossEntropyLoss(reduction = REDUCTION).to(DEVICE)\n        print(\"Custiom LOSSES\")\n    else:\n        RETURN_LOSSES = BASE_LOSSES(reduction = REDUCTION).to(DEVICE)\n    return RETURN_LOSSES","metadata":{"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class build_default_model(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        if USE_BASE_TIMM_MODEL:\n            self.model = timm.create_model(BASE_MODEL, pretrained = LOAD_BASE_WEIGHTS)\n        else:\n            self.model = BASE_MODEL(pretrained = LOAD_BASE_WEIGHTS)\n\n        if not BASE_MODEL_TRAINABLE:\n            for param in self.model.parameters():\n                param.requires_grad = False\n\n        if INCLUDE_TOP:\n            # Alternatively, it can be generalized to nn.Linear(num_ftrs, CLASSES)\n            if USE_BASE_TIMM_MODEL:\n                n_features = self.model.classifier.in_features\n                self.model.classifier = nn.Linear(n_features, CLASSES)\n            else:\n                n_features = self.model.fc.in_features\n                self.model.fc = nn.Linear(n_features, CLASSES, bias = BIAS)\n        else:\n            if USE_BASE_TIMM_MODEL:\n                n_features = self.model.classifier.in_features\n                self.model.classifier = nn.Sequential(\n                    nn.Dropout(DROPOUT),\n                    nn.Linear(n_features, CLASSES, bias = BIAS)\n                )\n            else:\n                n_features = self.model.fc.in_features\n                self.model.fc = nn.Sequential(\n                    nn.Dropout(DROPOUT),\n                    nn.Linear(n_features, CLASSES, bias = BIAS)\n                )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x","metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class build_custiom_model(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","metadata":{"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# 6. 定義回調函數方法 <a class=\"anchor\" id=\"6\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"def get_callbacks(optimizer):\n    if CALLBACKS_STEPLR:\n        # 等間隔調整學習率，調整倍數為gamma倍，調整間隔為step_size。間隔單位是step。需要注意的是，step通常是指epoch，不要弄成iteration了。\n        callbacks_scheduler = lr_scheduler.StepLR(optimizer, step_size = STEP_SIZE, gamma = GAMMA)\n    elif CALLBACKS_REDUCELRONPLATEAU:\n        # 當某指標不再變化（下降或升高），調整學習率，這是非常實用的學習率調整策略。例如，當驗證集的loss不再下降時，進行學習率調整；\n        # 或者監測驗證集的accuracy，當accuracy不再上升時，則調整學習率。\n        callbacks_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,  mode = MODE, factor = FACTOR, patience = PATIENCE, \n                                                   threshold = THRESHOLD, threshold_mode = THRESHOLD_MODE, cooldown = COOLDOWN, \n                                                   min_lr = MIN_LR, eps = SCHEDULER_EPS, verbose = SCHEDULER_VERBOSE)\n    elif CALLBACKS_COSINEANNEALINGWARMRESTAERS:\n        # 使用餘弦退火時間表設置每個參數組的學習率\n        callbacks_scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = T_0, T_mult = T_MULT, eta_min = ETA_MIN, \n                                                             verbose = SCHEDULER_VERBOSE)\n    else:\n        callbacks_scheduler = None \n    return callbacks_scheduler","metadata":{"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# 7. 製作資料集＆資料擴增&訓練模型 <a class=\"anchor\" id=\"7\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, df, one_hot_label = False, transforms = None):\n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        self.one_hot_label = one_hot_label\n        self.transforms = transforms\n        self.labels = self.df[LABEL_NAME].values\n        \n        if one_hot_label is True:\n            self.labels = np.eye(self.df[LABEL_NAME].max()+1)[self.labels]\n        \n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, index: int):\n        label = self.labels[index]\n        image_name = self.df[IMAGE_NAME].values[index]\n        if IMAGE_NAME_HAVE_EXTENSION:\n            image_path = TRAIN_DATA_PATH + image_name\n        else:\n            image_path = TRAIN_DATA_PATH + image_name + IMAGE_NAME_EXTENSION\n        image = cv2.imread(image_path)\n        \n        if COLOR_CONVERT_RGB:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.transforms is not None:\n            image = self.transforms(image = image)['image']\n            \n        return image, label","metadata":{"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# 確定是否將應用此增強。機率為 p = 1.0 意味著我們總是從上面應用轉換。\n# p = 0 將意味著將忽略轉換塊。\n# 0 < p < 1.0 等於每個擴增都具有以一定概率應用的選項。\n# OneOf 隨機選取一種增強擴增\n\ndef get_train_transforms():\n    return A.Compose([\n        A.Blur(blur_limit = BLUR_LIMIT, \n               p = P_BLUR), # 模糊\n        A.HorizontalFlip(p = P_HORIZONTALFLIP), # 水平翻轉\n        A.VerticalFlip(p = P_VERTICALFLIP), # 垂直翻轉\n        A.Flip(p = P_FLIP), # 水平和垂直翻轉\n        A.Resize(height = RESIZE_HEIGHT, \n                 width = RESIZE_WIDTH, \n                 p = P_RESIZE), # 縮放\n        A.RandomResizedCrop(height = RANDOMRESIZEDCROP_HEIGHT, \n                            width = RANDOMRESIZEDCROP_WIDTH, \n                            scale = RANDOMRESIZEDCROP_SCALE, \n                            p = P_RANDOMRESIZEDCROP), #隨機縮放剪裁\n        A.RandomRotate90(p = P_RANDOMROTATE90), # 隨機旋轉90度\n        A.ShiftScaleRotate(shift_limit = SHIFTSCALEROTATE_SHIFT_LIMIT, \n                           scale_limit = SHIFTSCALEROTATE_SCALE_LIMIT, \n                           rotate_limit = SHIFTSCALEROTATE_ROTATE_LIMIT, \n                           p = P_SHIFTSCALEROTATE), # 平移縮放旋轉\n        A.ElasticTransform(alpha = ELATICTRANSFORM_ALPHA, \n                           sigma = ELATICTRANSFORM_SIGMA, \n                           alpha_affine = ELATICTRANSFORM_ALPHA_AFFINE, \n                           p = P_ELATICTRANSFORM), # 彈性變換\n        A.GridDistortion(num_steps = GRIDDISTORTION_NUM_STEPS, \n                         p = P_GRIDDISTORTION), # 網格失真\n        A.RandomBrightnessContrast(brightness_limit = RANDOMBRIGHTNESSCONTRAST_BRIGHTNESS_LIMIT, \n                                   contrast_limit = RANDOMBRIGHTNESSCONTRAST_CONTRAST_LIMIT, \n                                   p = P_RANDOMBRIGHTNESSCONTRAST_CONTRAST), # 隨機亮度對比度\n        A.HueSaturationValue(hue_shift_limit = HUESATURATIONVALUE_HUE_SHIFT_LIMIT, \n                             sat_shift_limit = HUESATURATIONVALUE_SAT_SHIFT_LIMIT, \n                             val_shift_limit = HUESATURATIONVALUE_VAL_SHIFT_LIMIT, \n                             p = P_HUESATURATIONVALUE), # 隨機色調飽和度值\n        A.CLAHE(clip_limit = CLAHE_CLIP_LIMIT, \n                p = P_CLAHE), # 將對比度受限的自適應直方圖均衡化應用於輸入圖像\n        A.Cutout(num_holes = COARSEDROPOUT_NUM_HOLES, \n                        max_h_size = COARSEDROPOUT_MAX_H_SIZE, \n                        max_w_size = COARSEDROPOUT_MAX_W_SIZE, \n                        p = P_COARSEDROPOUT), # 隨機在圖像上生成黑色矩形\n        A.Normalize(\n             mean = NORMALIZE_MEAN, \n             std = NORMALIZE_STD, \n            max_pixel_value = NORMALIZE_MAX_PIXEL_VALUE, \n            p = P_NORMALIZE), # 正規化。\n        ToTensorV2(p = P_TOTENSORV2) # 歸一化\n    ], p = P_TRAIN_TRANSFORMS)\n\ndef get_val_transforms():\n    return A.Compose([\n        A.Resize(height = RESIZE_HEIGHT, \n                 width = RESIZE_WIDTH, \n                 p = P_RESIZE), # 縮放\n        A.Normalize(\n             mean = NORMALIZE_MEAN,\n             std = NORMALIZE_STD, \n            max_pixel_value = NORMALIZE_MAX_PIXEL_VALUE, \n            p = P_NORMALIZE), # 正規化。\n        ToTensorV2(p = P_TOTENSORV2) # 歸一化\n    ], p = P_VAL_TRANSFORMS)","metadata":{"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def prepare_dataloader(fold, df, train_index, valid_index):\n    \n    if FOLD >1:\n        train_data = df.loc[train_index,:]\n        validation_data = df.loc[valid_index,:]\n    else:\n        X_train, X_val, Y_train, Y_val = train_test_split(train_csv[IMAGE_NAME], \n                                                              train_csv[LABEL_NAME], \n                                                              test_size = DATA_SPLIT, \n                                                              random_state = SEED)\n        train_data = pd.DataFrame(X_train)\n        train_data.columns = [IMAGE_NAME]\n        train_data[LABEL_NAME] = Y_train\n\n        validation_data = pd.DataFrame(X_val)\n        validation_data.columns = [IMAGE_NAME]\n        validation_data[LABEL_NAME] = Y_val\n        \n    train_dataset = MyDataset(train_data, one_hot_label = ONE_HOT_LABEL, transforms = get_train_transforms())\n    val_dataset = MyDataset(validation_data, one_hot_label = ONE_HOT_LABEL, transforms = get_val_transforms())\n    \n    # 紀錄訓練集跟驗證集大小\n    train_dataset_size = len(train_dataset)\n    val_dataset_size = len(val_dataset)\n    \n    #for metrics\n    dataset_sizes = { 'train': train_dataset_size, 'val': val_dataset_size}\n    print(dataset_sizes)\n    \n    train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE[fold], pin_memory = PIN_MEMORY, \n                                               shuffle = True, num_workers = NUM_WORKERS, drop_last = DROP_LAST)\n    \n    val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE[fold], pin_memory = PIN_MEMORY, \n                                               shuffle = False, num_workers = NUM_WORKERS)\n    \n    return train_loader, val_loader, train_dataset_size, val_dataset_size","metadata":{"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#save the losses and metrics for further visualization\ntrain_metrics = {PLOT_METRICS:[], 'loss':[]}\nval_metrics = {PLOT_METRICS:[], 'loss':[]}\n\n# 宣告為訓練後混淆矩陣預測\nall_labels = []; all_outputs = []","metadata":{"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def display_training_curves(fold, skf):\n    plt.figure(figsize=(TRAINING_CURVES_FIGSIZE_W,TRAINING_CURVES_FIGSIZE_H))\n    plt.plot(np.arange(EPOCHS[fold]),train_metrics[PLOT_METRICS],'-o',label='TRAIN '+PLOT_METRICS.upper(),color='#ff7f0e')\n    plt.plot(np.arange(EPOCHS[fold]),val_metrics[PLOT_METRICS],'-o',label='VALIDATION '+PLOT_METRICS.upper(),color='#1f77b4')\n    x = np.argmax( val_metrics[PLOT_METRICS] ); y = np.max( val_metrics[PLOT_METRICS] )\n    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=TRAINING_CURVES_SCATTER_SCALAR,color='#1f77b4')\n    plt.text(x-TRAINING_CURVES_SCATTER_METRICS_TEXT_XSCALAR*xdist,y-TRAINING_CURVES_SCATTER_METRICS_TEXT_YSCALAR*ydist,'max '+PLOT_METRICS+'\\n%.4f'%y,size=TRAINING_CURVES_SCATTER_TEXTSIZE)\n    plt.ylabel(PLOT_METRICS.upper(),size=TRAINING_CURVES_YLABEL_FONTSIZE); plt.xlabel('EPOCH',size=TRAINING_CURVES_XLABEL_FONTSIZE)\n    plt.grid(alpha=TRAINING_CURVES_GRID_ALPHA)\n    plt.legend(loc=2)\n    plt2 = plt.gca().twinx()\n    plt2.plot(np.arange(EPOCHS[fold]),train_metrics['loss'],'-o',label='TRAIN LOSS',color='#2ca02c')\n    plt2.plot(np.arange(EPOCHS[fold]),val_metrics['loss'],'-o',label='VALIDATION LOSS',color='#d62728')\n    x = np.argmin( val_metrics['loss'] ); y = np.min( val_metrics['loss'] )\n    ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=TRAINING_CURVES_SCATTER_SCALAR,color='#d62728')\n    plt.text(x-TRAINING_CURVES_SCATTER_LOSS_TEXT_XSCALAR*xdist,y+TRAINING_CURVES_SCATTER_LOSS_TEXT_YSCALAR*ydist,'min loss\\n%.4f'%y,size=TRAINING_CURVES_SCATTER_TEXTSIZE)\n    plt.ylabel('LOSS',size=TRAINING_CURVES_YLABEL_FONTSIZE)\n    if skf:\n        plt.title('FOLD %i - IMAGE SIZE %i, %s'%\n                  (fold+1, IMAGE_SIZE[fold], MODEL_NAME.upper()), size=TRAINING_CURVES_TITLE_FONTSIZE)\n    else:\n        plt.title(' IMAGE SIZE %i, %s'%\n                  (IMAGE_SIZE[fold], MODEL_NAME.upper()), size=TRAINING_CURVES_TITLE_FONTSIZE)\n    plt.grid(alpha=TRAINING_CURVES_GRID_ALPHA)\n    plt.legend(loc=3)\n    plt.show()","metadata":{"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(fold, epoch, model, scaler, train_loss, optimizer, train_loader, train_dataset_size, \n                    scheduler = None, scheduler_batch_update = False):\n    # 設定模型為訓練模式\n    model.train()\n    \n    # 計算迭代目前的step\n    epoch_step = 0\n\n    # 計算當下loss\n    running_loss = 0.0\n    \n    outputs_all = []\n    labels_all = []\n\n    # 遍歷enumaretad批處理\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader), \n                position = TQDM_POSITION, leave = TQDM_LEAVE)\n    for batch_idx, (inputs, labels) in pbar:\n        # 提取輸入和標籤\n        inputs = inputs.to(DEVICE).float()\n        labels = labels.to(DEVICE).long()\n\n        # 前向過程(model + loss)開啟 autocast\n        with autocast():\n            outputs = model(inputs)\n            loss = train_loss(outputs, labels)\n            \n        # 歸一化損失以說明批次累積\n        loss = loss / ACCUM_ITER \n            \n        # Scales loss. 為了梯度放大\n        # 反向傳播在autocast上下文之外\n        scaler.scale(loss).backward()\n\n        # 權重更新\n        if ((batch_idx + 1) %  ACCUM_ITER == 0) or ((batch_idx + 1) == len(train_loader)):\n\n            # scaler.step() 首先把梯度的值unscale回來.\n            # 如果梯度的值不是 infs 或者 NaNs, 那麼調用optimizer.step()來更新權重,\n            # 否則，忽略step調用，從而保證權重不更新（不被破壞）\n            scaler.step(optimizer)\n            \n            # 準備著，看是否要增大scaler\n            scaler.update()\n            \n            # 零參數梯度\n            optimizer.zero_grad() \n            \n            if scheduler is not None and scheduler_batch_update:\n                scheduler.step()\n            \n        if CALCULATE_EPOCH_STEP:\n            # step / epoch statistics\n            epoch_step += 1 \n            epoch_step_outputs_all = np.concatenate([torch.argmax(outputs, 1).detach().cpu().numpy()])\n            epoch_step_labels_all = np.concatenate([labels.detach().cpu().numpy()])  \n            train_metrics['loss'].append(loss.item())   \n            if PLOT_METRICS == \"accuracy\":\n                epoch_step_accuracy = (epoch_step_outputs_all == epoch_step_labels_all).mean()\n                if not PLOT_EPOCH_AVERAGE:\n                    train_metrics[PLOT_METRICS].append(epoch_step_accuracy)\n                print(f\"Step {epoch_step} / Epoch {epoch+1} - Train Loss: {loss.item():.4f}, Train Accuracy: {epoch_step_accuracy:.4f}\")\n            else:\n                epoch_step_auc =  roc_auc_score(epoch_step_labels_all, epoch_step_outputs_all)\n                if not PLOT_EPOCH_AVERAGE:\n                    train_metrics[PLOT_METRICS].append(epoch_step_auc)\n                print(f\"Step {epoch_step} / Epoch {epoch+1} - Train Loss: {loss.item():.4f}, Train Auc: {epoch_step_auc:.4f}\")\n                \n        # epoch statistics\n        running_loss += loss.item()*inputs.size(0)\n        outputs_all += [torch.argmax(outputs, 1).detach().cpu().numpy()]\n        labels_all += [labels.detach().cpu().numpy()]    \n    outputs_all = np.concatenate(outputs_all)\n    labels_all = np.concatenate(labels_all)\n    \n    if CALCULATE_EPOCH_AVERAGE:\n        epoch_loss = running_loss / train_dataset_size\n        train_metrics['loss'].append(epoch_loss)\n        if PLOT_METRICS == \"accuracy\":\n            epoch_accuracy = (outputs_all == labels_all).mean()\n            if PLOT_EPOCH_AVERAGE:\n                train_metrics[PLOT_METRICS].append(epoch_accuracy)\n            print(f\"Epoch {epoch+1}/{EPOCHS[fold]} - Train Average Loss: {epoch_loss:.4f}, Train Average Accuracy: {epoch_accuracy:.4f}\")\n        elif PLOT_METRICS == \"auc\":\n            epoch_auc =  roc_auc_score(labels_all, outputs_all)\n            if PLOT_EPOCH_AVERAGE:\n                train_metrics[PLOT_METRICS].append(epoch_auc)\n            print(f\"Epoch {epoch+1}/{EPOCHS[fold]} - Train Average Loss: {epoch_loss:.4f}, Train Average Auc: {epoch_auc:.4f}\")\n       \n    if scheduler is not None and not scheduler_batch_update:\n        scheduler.step()\n        \ndef valid_one_epoch(fold, epoch, model, val_loss, val_loader, val_dataset_size, monitor_metrics, save_model_path, \n                    scheduler = None, scheduler_loss_update = False):\n    # 設定模型為評估模式\n    model.eval()\n    \n    # 計算迭代目前的step\n    epoch_step = 0\n\n    # 計算每批loss\n    running_loss = 0.0\n    \n    # 計算每迭代loss\n    epoch_loss = 0.0\n    \n    # 計算每迭代accuracy\n    epoch_accuracy = 0.0\n    \n    # 計算每迭代auc\n    epoch_auc = 0.0\n    \n    # 計算提早停止次數\n    early_stopping = 0\n    \n    outputs_all = []\n    labels_all = []\n    \n    # 遍歷enumaretad批處理\n    pbar = tqdm(enumerate(val_loader), total=len(val_loader), \n                position = TQDM_POSITION, leave = TQDM_LEAVE)\n    for batch_idx, (inputs, labels) in pbar:\n        # 提取輸入和標籤\n        inputs = inputs.to(DEVICE).float()\n        labels = labels.to(DEVICE).long()     \n        outputs = model(inputs)\n        loss = val_loss(outputs, labels)\n        \n        if CALCULATE_EPOCH_STEP:\n            # step / epoch statistics\n            epoch_step += 1            \n            epoch_step_outputs_all = np.concatenate([torch.argmax(outputs, 1).detach().cpu().numpy()])\n            epoch_step_labels_all = np.concatenate([labels.detach().cpu().numpy()])\n            val_metrics['loss'].append(loss.item())    \n            if PLOT_METRICS == \"accuracy\":\n                epoch_step_accuracy = (epoch_step_outputs_all == epoch_step_labels_all).mean()\n                if not PLOT_EPOCH_AVERAGE:\n                    val_metrics[PLOT_METRICS].append(epoch_step_accuracy)\n                print(f\"Step {epoch_step} / Epoch {epoch+1} - Val Loss: {loss.item():.4f}, Val Accuracy: {epoch_step_accuracy:.4f}\")\n            else:\n                epoch_step_auc =  roc_auc_score(epoch_step_labels_all, epoch_step_outputs_all)\n                if not PLOT_EPOCH_AVERAGE:\n                    val_metrics[PLOT_METRICS].append(epoch_step_accuracy)\n                print(f\"Step {epoch_step} / Epoch {epoch+1} - Val Loss: {loss.item():.4f}, Val Auc: {epoch_step_auc:.4f}\")\n                \n        # epoch statistics\n        running_loss += loss.item()*inputs.size(0)\n        outputs_all += [torch.argmax(outputs, 1).detach().cpu().numpy()]\n        labels_all += [labels.detach().cpu().numpy()]      \n    outputs_all = np.concatenate(outputs_all)\n    labels_all = np.concatenate(labels_all)\n    \n    # 為了計算全部迭代混淆矩陣\n    all_outputs.append(outputs_all)\n    all_labels.append(labels_all)\n        \n    if CALCULATE_EPOCH_AVERAGE:\n        epoch_loss = running_loss / val_dataset_size\n        val_metrics['loss'].append(epoch_loss)\n        if PLOT_METRICS == \"accuracy\":\n            epoch_accuracy = (outputs_all == labels_all).mean()\n            if PLOT_EPOCH_AVERAGE:\n                val_metrics[PLOT_METRICS].append(epoch_accuracy)\n            print(f\"Epoch {epoch+1}/{EPOCHS[fold]} - Val Average Loss: {epoch_loss:.4f}, Val Average Accuracy: {epoch_accuracy:.4f}\")         \n        elif PLOT_METRICS == \"auc\":\n            epoch_auc =  roc_auc_score(labels_all, outputs_all)\n            if PLOT_EPOCH_AVERAGE:\n                val_metrics[PLOT_METRICS].append(epoch_auc)\n            print(f\"Epoch {epoch+1}/{EPOCHS[fold]} - Val Average Loss: {epoch_loss:.4f}, Val Average Auc: {epoch_auc:.4f}\")\n        \n    if CALLBACKS_CHECK_POINTER:\n        save_model = False\n        if SAVE_BEST_ONLY:\n            if (epoch_loss < monitor_metrics or monitor_metrics == 0) and MONITOR == \"val_loss\":\n                monitor_metrics = epoch_loss\n                save_model = True\n            elif epoch_accuracy > monitor_metrics and MONITOR == \"val_acc\" and PLOT_METRICS == \"accuracy\":\n                monitor_metrics = epoch_accuracy\n                save_model = True \n            elif epoch_auc > monitor_metrics and MONITOR == \"val_auc\" and PLOT_METRICS == \"auc\":\n                monitor_metrics = epoch_auc\n                save_model = True\n            else:\n                early_stopping += 1\n        else:\n            if MONITOR == \"val_loss\":\n                if monitor_metrics is not 0:\n                    if epoch_loss >= monitor_metrics:\n                        early_stopping += 1\n                monitor_metrics = epoch_loss\n            elif MONITOR == \"val_acc\":\n                if monitor_metrics is not 0:\n                    if epoch_accuracy <= monitor_metrics:\n                        early_stopping += 1\n                monitor_metrics = epoch_accuracy\n            elif MONITOR == \"val_auc\":\n                if monitor_metrics is not 0:\n                    if epoch_auc <= monitor_metrics:\n                        early_stopping += 1\n                monitor_metrics = epoch_auc\n            save_model = True\n            \n        if SAVE_WEIGHTS_ONLY and save_model:\n            torch.save(model.state_dict(), save_model_path)\n            print('Save weights')\n        elif not SAVE_WEIGHTS_ONLY and save_model:\n            torch.save(model, save_model_path)\n            print('Save model')\n\n    if scheduler is not None and VAL_ENABLE_SCHEDULER:\n        if scheduler_loss_update:\n            scheduler.step(epoch_loss)\n        else:\n            scheduler.step()\n            \n    return monitor_metrics, early_stopping","metadata":{"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def train_process(fold, skf, train_index, valid_index):\n    if skf:\n        print('FOLD %i - IMAGE SIZE %i WITH %s AND BATCH_SIZE %i'%(fold+1,IMAGE_SIZE[fold],MODEL_NAME.upper(),BATCH_SIZE[fold]))\n    else:\n        print('IMAGE SIZE %i WITH %s AND BATCH_SIZE %i'%(IMAGE_SIZE[fold],MODEL_NAME.upper(),BATCH_SIZE[fold]))\n        \n    if skf:\n        # (String)訓練模型FOLD>1的儲存路徑\n        SAVE_MODEL_PATH = PROJECT_PATH+r'/models/'+MODEL_NAME+'_fold_%i.pt'%(fold+1)\n    else:\n        SAVE_MODEL_PATH = TRAIN_MODEL_PATH\n    \n    train_loader, val_loader, train_dataset_size, val_dataset_size = prepare_dataloader(fold, train_csv, train_index, valid_index)\n    \n    # 載入模型或權重\n    if LOAD_MODEL:\n        # load model\n        model = torch.load(LOAD_MODEL_PATH)\n    elif USE_BASE_MODEL:\n        # 創建model，默認是torch.FloatTensor\n        model = build_default_model()\n    else:\n        # ==== INIT CUSTIOM MODEL\n        model = build_custiom_model()\n        if LOAD_WEIGHTS:\n            # load model weights\n            model.load_state_dict(LOAD_WEIGHTS_PATH)\n\n    model.to(DEVICE)\n    optimizer = build_optimizers(model)\n    \n    # 在訓練最開始之前實例化一個GradScaler對象\n    scaler = GradScaler()\n    \n    scheduler = get_callbacks(optimizer) # 回調函式\n\n    train_loss = build_losses() # train loss\n    val_loss = build_losses() # val loss\n\n    if MODEL_PRINT:\n        # Print model's state_dict\n        print(\"Model's state_dict:\")\n        for param_tensor in model.state_dict():\n            print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n\n    if OPTIMIZER_PRINT:\n        # Print optimizer's state_dict\n        print(\"Optimizer's state_dict:\")\n        for var_name in optimizer.state_dict():\n            print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n            \n    # 紀錄最好指標\n    monitor_metrics = 0.0\n               \n    for epoch in range(EPOCHS[fold]):\n        train_one_epoch(fold, epoch, model, scaler, train_loss, optimizer, train_loader, train_dataset_size, scheduler = scheduler, \n                        scheduler_batch_update = SCHEDULER_BATCH_UPDATE)\n\n        with torch.no_grad():\n            monitor_metrics, early_stopping = valid_one_epoch(fold, epoch, model, val_loss, val_loader, val_dataset_size, monitor_metrics, SAVE_MODEL_PATH, \n                                              scheduler = scheduler, scheduler_loss_update = SCHEDULER_LOSS_UPDATE)\n\n        if CALLBACKS_EARLY_STOPPING:\n            if early_stopping >= PATIENCE_ELS:\n                break\n            \n    display_training_curves(fold, skf)\n    \n    if not CALLBACKS_CHECK_POINTER:\n        if SAVE_WEIGHTS_ONLY:\n            torch.save(model.state_dict(), SAVE_MODEL_PATH)\n            print('Save weights')\n        else:\n            torch.save(model, SAVE_MODEL_PATH)\n            print('Save model')\n    \n    del model, optimizer, train_loader, val_loader, scaler, scheduler\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    try:\n        print('Training start')\n        since = time.time()\n        if LOAD_CSV:\n            if FOLD > 1:\n                for fold,(train_index, valid_index) in enumerate(SKF.split(np.arange(train_csv.shape[0]), train_csv[LABEL_NAME])):\n                    train_process(fold = fold, skf = True, train_index = train_index, valid_index = valid_index)\n            else:\n                train_process(fold = 0, skf = False, train_index = None, valid_index = None)  \n        time_elapsed = time.time() - since\n        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))  \n    except Exception as exception:\n        print(exception)\n        raise","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training start\nIMAGE SIZE 224 WITH RESNET18 AND BATCH_SIZE 512\n{'train': 2929, 'val': 733}\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/44.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56951347bf02470cbbc299e8ddfd8e56"}},"metadata":{}},{"name":"stderr","text":" 17%|█▋        | 1/6 [01:44<08:41, 104.31s/it]","output_type":"stream"},{"name":"stdout","text":"Step 1 / Epoch 1 - Train Loss: 2.1600, Train Accuracy: 0.1094\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 2/6 [03:15<06:25, 96.30s/it] ","output_type":"stream"},{"name":"stdout","text":"Step 2 / Epoch 1 - Train Loss: 1.7933, Train Accuracy: 0.2090\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 3/6 [04:43<04:37, 92.62s/it]","output_type":"stream"},{"name":"stdout","text":"Step 3 / Epoch 1 - Train Loss: 1.6302, Train Accuracy: 0.3281\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 4/6 [06:10<03:00, 90.32s/it]","output_type":"stream"},{"name":"stdout","text":"Step 4 / Epoch 1 - Train Loss: 1.5696, Train Accuracy: 0.3809\n","output_type":"stream"},{"name":"stderr","text":" 83%|████████▎ | 5/6 [07:37<01:29, 89.41s/it]","output_type":"stream"},{"name":"stdout","text":"Step 5 / Epoch 1 - Train Loss: 1.5149, Train Accuracy: 0.4238\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6/6 [08:41<00:00, 86.93s/it]\n  0%|          | 0/2 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Step 6 / Epoch 1 - Train Loss: 1.4914, Train Accuracy: 0.4607\nEpoch 1/1 - Train Average Loss: 1.7031, Train Average Accuracy: 0.3117\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 8. 混淆矩陣 & Quadratic Weighted Kappa<a class=\"anchor\" id=\"8\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"cm_correct_labels = np.concatenate(all_labels)\ncm_predictions = np.concatenate(all_outputs)\nprint(\"Correct   labels: \", cm_correct_labels.shape, cm_correct_labels)\nprint(\"Predicted labels: \", cm_predictions.shape, cm_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''混淆矩陣包含四個要素:TP(True Positive)正確預測成功的正樣本, TN(True Negative)正確預測成功的負樣本, \nFP(False Positive)錯誤預測成正樣本，實際上為負樣本, FN(False Negative)錯誤預測成負樣本(或者說沒能預測出來的正樣本)'''\ncm = confusion_matrix(cm_correct_labels, cm_predictions)\ncm = cm/cm.sum(axis=1)[:, np.newaxis]\n\nf,ax = plt.subplots(figsize = (CONFUSION_MATRIX_FIGSIZE_W, CONFUSION_MATRIX_FIGSIZE_H))\nsns.heatmap(cm, annot = True, ax = ax, annot_kws={\"size\": CONFUSION_MATRIX_HEATMAP_FONTSIZE})\nplt.title(\"CONFUSION MATRIX\", fontsize=CONFUSION_MATRIX_TITLE_FONTSIZE)\nplt.xlabel(\"PREDICTED\", fontsize=CONFUSION_MATRIX_XLABEL_FONTSIZE)\nplt.ylabel(\"TRUE\", fontsize=CONFUSION_MATRIX_YLABEL_FONTSIZE)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nAccuracy = (TP+TN)/(TP+FP+TN+FN)\nPrecision(準確率) = TP/(TP+FP)\nRecall(召回率) = TP/(TP+FN)\nF1-score(Recall與Precision的調和平均數) = 2 * Precision * Recall / (Precision + Recall)\n'''\naccuracy = accuracy_score(cm_correct_labels, cm_predictions)\nprecision = precision_score(cm_correct_labels, cm_predictions, labels = range(CLASSES), average = AVERAGE)\nrecall = recall_score(cm_correct_labels, cm_predictions, labels = range(CLASSES), average = AVERAGE)\nscore = f1_score( cm_correct_labels, cm_predictions, labels = range(CLASSES), average = AVERAGE)\nprint('Accuracy: {:.3f}, Precision: {:.3f}, Recall: {:.3f}, F1 score: {:.3f}'.format(accuracy, precision, recall, score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification report on training Data\nprint(classification_report(cm_correct_labels, cm_predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quadratic Weighted Kappa\ncohen_kappa_score(cm_predictions, cm_correct_labels, weights='quadratic')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. 待辦事項<a class=\"anchor\" id=\"9\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"markdown","source":"TensorBoard","metadata":{}}]}