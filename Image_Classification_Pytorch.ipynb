{"metadata":{"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image_Classification_Pytorch\nhttps://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html","metadata":{}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# Table of Contents\n\n1. [套件安裝與載入](#1)\n1. [環境檢測與設定](#2)\n1. [開發參數設定](#3)\n1. [資料處理](#4)\n    -  [載入CSV檔](#4.1)\n    -  [檢查CSV檔缺失值](#4.2)\n1. [定義模型方法](#5)\n1. [定義回調函數方法](#6)\n1. [製作資料集＆資料擴增&訓練模型](#7)\n1. [混淆矩陣](#8)\n1. [提交](#9)\n1. [待辦事項](#10)","metadata":{}},{"cell_type":"markdown","source":"# 1. 套件安裝與載入<a class=\"anchor\" id=\"1\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"# 資料處理套件\nimport os\nimport gc\nimport sys\nimport random\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom albumentations import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 設定顯示中文字體\nfrom matplotlib.font_manager import FontProperties\nplt.rcParams['font.sans-serif'] = ['Microsoft JhengHei'] # 用來正常顯示中文標籤\nplt.rcParams['font.family'] = 'AR PL UMing CN'\nplt.rcParams['axes.unicode_minus'] = False # 用來正常顯示負號","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pytorch深度學習模組套件\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom torch.utils.data import Dataset,DataLoader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. 環境檢測與設定<a class=\"anchor\" id=\"2\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDEVICE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''執行環境參數設定'''\n\n# (Boolean)是否為本機\nLOCAL = False\n\n# (Boolean)是否為 Colab\nCOLAB = False\n\n\n'''檔案路徑參數設定'''\n\n# (String)Root路徑\nif LOCAL:\n    PATH = r'../'\nelif COLAB:\n    PATH = r'/content/drive/My Drive/Colab Notebooks/'\nelse:\n    PATH = r'../input/'\n    OUTPUT_PATH = r'/kaggle/working/'\n    \n# (String)資料根路徑\nDATA_ROOT_PATH = PATH+r'datasets/AI_CUP_2020_AIMango_Defective_Classification/' \n\n# (String)訓練資料路徑\nTRAIN_DATA_PATH = DATA_ROOT_PATH+r'C1-P2_Train Dev/Train'\n\n# (String)訓練CSV路徑，如為None則不讀CSV檔\nTRAIN_CSV_PATH = DATA_ROOT_PATH+r'C1-P2_Train Dev/train.csv'\n\n# (String)測試資料路徑\nTEST_DATA_PATH = DATA_ROOT_PATH+r'C1-P2_Train Dev/Test'\n\n# (String)測試CSV路徑，如為None則不讀CSV檔\nTEST_CSV_PATH = DATA_ROOT_PATH+r'test_example.csv'\n\n# (String)專案名稱\nPROJECT_NAME = 'AI_CUP_2020_AIMango_Defective_Classification'\n\n# (String)專案檔案儲存路徑\nif LOCAL or COLAB:\n    OUTPUT_PATH = PATH\nPROJECT_PATH = OUTPUT_PATH+PROJECT_NAME+'/'+PROJECT_NAME+' '+datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n\n# (String)權重名稱(使用哪個權重)\nWEIGHTS_NAME = 'efficientnetb7'\n\n# (String)模型名稱(使用哪個模型)\nMODEL_NAME = 'efficientnetb7'\n\n# (String)讀取預訓練權重的儲存路徑 \nLOAD_WEIGHTS_PATH = PROJECT_PATH+r'/models/backup/'+WEIGHTS_NAME+'.pth'\n\n# (String)讀取預訓練模型的儲存路徑 \nLOAD_MODEL_PATH = PROJECT_PATH+r'/models/backup/'+MODEL_NAME+'.pth'\n\n# (String)訓練模型的儲存路徑\nTRAIN_MODEL_PATH = PROJECT_PATH+r'/models/'+MODEL_NAME+'.pth'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not LOCAL and COLAB:\n    from google.colab import drive\n    drive.mount('/content/drive')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEVICE != \"CPU\":\n    !nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.isfile(TRAIN_CSV_PATH) and os.path.isfile(TEST_CSV_PATH):\n    LOAD_CSV = True\nelse:\n    LOAD_CSV = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.isdir(PROJECT_PATH+r'/models/'):\n    os.makedirs(PROJECT_PATH+r'/models/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. 開發參數設定<a class=\"anchor\" id=\"3\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"'''客製參數設定'''\n\n\n'''資料參數設定'''\n\n# (Int)分類數量\nCLASSES = 3\n\n# (Int)有CSV檔該參數才有用，1則為不做交叉驗證\nFOLD = 1\n\n# (Int)沒CSV檔，FOLD該參數固定為1\nif not LOAD_CSV:\n    FOLD = 1\n    \n# (Int)圖片尺寸\nIMAGE_SIZE = [224]*FOLD\n\n# (String)圖片副檔名\nIMAGE_NAME_EXTENSION = '.jpg'\n\n# (String)CSV圖片檔名欄位(不包含路徑)\nIMAGE_NAME = 'image_id'\n\n# (String)CSV圖片檔名欄位(包含路徑)\nIMAGE_NAME_ROOT = 'image'\n\n# (String)CSV標籤欄位\nLABEL_NAME = 'grade'\n\n# (String)CSV標籤欄位類型\nLABEL_NAME_TYPE = 'string'\n\n# (Boolean)CSV圖片檔名欄位是否包含副檔名\nIMAGE_NAME_HAVE_EXTENSION = True\n\n# (Int)不包含副檔名的圖片檔名長度，因為CSV檔名欄位有副檔名時需要移除\nIMAGE_NAME_LENGTH = 5\n\n# (String)測試集CSV標籤欄位\nTEST_LABEL_NAME = 'label'\n\n# (String)測試集CSV儲存檔名\nTEST_CSV_FILE_NAME = 'submission.csv'\n\n# (Int)不同的種子會產生不同的Random或分層K-FOLD分裂, 42則是預設固定種子\nSEED = 42\n\n# (Boolean)如為True每次返回的卷積算法將是確定的，即默認算法\nCUDNN_DETERMINISTIC = True\n\n# (Boolean)PyTorch 中對模型裡的卷積層進行預先的優化，也就是在每一個卷積層中測試 cuDNN 提供的所有卷積實現算法，\n# 然後選擇最快的那個。這樣在模型啟動的時候，只要額外多花一點點預處理時間，就可以較大幅度地減少訓練時間\nCUDNN_BENCHMARK = True\n\n\n'''資料擴增參數設定'''\n\n\n''''模型參數設定'''\n\n# (Boolean)是否印出完整模型\nMODEL_PRINT = True\n\n\n''''回調函數參數設定'''\n\n\n''''編譯參數設定'''\n\n# (Float)優化器學習率 \nLEARNING_RATE = 1e-3\n\n# (String)優化器指定，None為客制，須另外撰寫\nBASE_OPTIMIZERS = optim.Adam\n\n# (String)損失函數，None為客制，須另外撰寫\nBASE_LOSSES = nn.CrossEntropyLoss\n\n# (String)指定還原成適用於輸出，預設mean\nREDUCTION = \"mean\"\n\n\n''''訓練參數設定'''\n\n# (Int List)每批訓練的尺寸\nBATCH_SIZE = [16]*FOLD\n\n# (Int)使用基於進程的線程時，要啟動的最大進程數。如果未指定，NUM_WORKERS則默認為1。\nNUM_WORKERS = 1\n\n# (Boolean)批次處理在大小不合適的情況下，是否刪除最後一個不完整的批次\nDROP_LAST = False\n\n\n''''圖表參數設定'''\n\n# (Float)全部SNS圖表的字形縮放\nALL_SNS_FONT_SCALE = 1.0\n\n# (Int)CSV缺失值圖表寬度\nCSV_COUNTPLOT_FIGSIZE_W = 10\n\n# (Int)CSV缺失值圖表高度\nCSV_COUNTPLOT_FIGSIZE_H = 10\n\n# (Int)CSV缺失值圖表標題字型大小\nCSV_COUNTPLOT_TITLE_FONTSIZE = 20\n\n# (Int)CSV缺失值圖表X軸標題字型大小\nCSV_COUNTPLOT_XLABEL_FONTSIZE = 15\n\n# (Int)CSV缺失值圖表Y軸標題字型大小\nCSV_COUNTPLOT_YLABEL_FONTSIZE = 15","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = CUDNN_DETERMINISTIC\n    torch.backends.cudnn.benchmark = CUDNN_BENCHMARK\n\nseed_everything(SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 設置sns圖表縮放係數\nsns.set(font_scale = ALL_SNS_FONT_SCALE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. 資料處理<a class=\"anchor\" id=\"4\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"markdown","source":"## 4.1 載入CSV檔 <a class=\"anchor\" id=\"4.1\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"if LOAD_CSV:\n    print('Reading data...')\n\n    # 讀取訓練資料集CSV檔\n    train_csv = pd.read_csv(TRAIN_CSV_PATH,encoding=\"utf8\")\n\n    # 讀取測試資料集CSV檔\n    test_csv = pd.read_csv(TEST_CSV_PATH,encoding=\"utf8\")\n\n    print('Reading data completed')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if LOAD_CSV:\n    # CSV檔名欄位包括副檔名時，需要移除副檔名\n    if IMAGE_NAME_HAVE_EXTENSION:\n        train_csv[IMAGE_NAME] = train_csv[IMAGE_NAME].str.slice(stop = IMAGE_NAME_LENGTH)\n        test_csv[IMAGE_NAME] = test_csv[IMAGE_NAME].str.slice(stop = IMAGE_NAME_LENGTH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if LOAD_CSV:\n    # 顯示訓練資料集CSV檔\n    print(train_csv.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if LOAD_CSV:\n    print(\"Shape of train_data :\", train_csv.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if LOAD_CSV:\n    # 顯示測試資料集CSV檔\n    print(test_csv.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if LOAD_CSV:\n    print(\"Shape of test_data :\", test_csv.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 檢查CSV檔缺失值 <a class=\"anchor\" id=\"4.2\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"if LOAD_CSV:\n    total = train_csv.isnull().sum().sort_values(ascending = False)\n    percent = (train_csv.isnull().sum()/train_csv.isnull().count()*100).sort_values(ascending = False)\n    missing_train_csv  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    print(missing_train_csv.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if LOAD_CSV:\n    print(train_csv[LABEL_NAME].value_counts())\n    f,ax = plt.subplots(figsize=(CSV_COUNTPLOT_FIGSIZE_W, CSV_COUNTPLOT_FIGSIZE_H))\n    sns.countplot(train_csv[LABEL_NAME], hue = train_csv[LABEL_NAME],ax = ax)\n    plt.title(\"LABEL COUNT\", fontsize=CSV_COUNTPLOT_TITLE_FONTSIZE)\n    plt.xlabel(LABEL_NAME.upper(), fontsize=CSV_COUNTPLOT_XLABEL_FONTSIZE)\n    plt.ylabel(\"COUNT\", fontsize=CSV_COUNTPLOT_YLABEL_FONTSIZE)\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. 定義模型方法<a class=\"anchor\" id=\"5\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"def build_optimizers():\n    if BASE_OPTIMIZERS == None:\n        print(\"Custiom OPTIMIZERS\")\n    else:\n        RETURN_OPTIMIZERS = BASE_OPTIMIZERS\n    return RETURN_OPTIMIZERS\n\noptimizer = build_optimizers()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_losses():\n    if BASE_LOSSES == None:\n        print(\"Custiom LOSSES\")\n    else:\n        RETURN_LOSSES = BASE_LOSSES\n    return RETURN_LOSSES\n\nloss = build_losses()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class build_model(nn.Module):\n\n    def __init__(self):\n        super(build_model, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n# ==== INIT MODEL    \nmodel = build_model()\nmodel.to(DEVICE)\noptimizer = optimizer(model.parameters(), lr=LEARNING_RATE)\n\n# Later we have to filter the invalid steps.\ncriterion = loss(reduction=REDUCTION)\n\nif MODEL_PRINT:\n    print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. 定義回調函數方法<a class=\"anchor\" id=\"6\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"markdown","source":"# 7. 製作資料集＆資料擴增&訓練模型 <a class=\"anchor\" id=\"7\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"# 待增加資料擴增類別\ntransform = transforms.Compose(\n    [transforms.Resize(size=IMAGE_SIZE[0]), # 縮放\n     transforms.ToTensor()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if LOAD_CSV:\n    class DatasetRetriever(Dataset):\n        def __init__(self, csv_path=None, fold=fold, transforms=None):\n            ids = pd.read_csv(csv_path).id.values\n            \n            kf = KFold(n_splits=nfolds,random_state=SEED,shuffle=True)\n            ids = set(ids[list(kf.split(ids))[fold][0 if train else 1]])\n            self.image_ids = [fname for fname in os.listdir(TRAIN) if fname.split('_')[0] in ids]\n            self.transforms = transforms\n\n        def __len__(self) -> int:\n            return len(self.image_ids)\n\n        def __getitem__(self, index: int):\n            image_id = self.image_ids[index]\n            image = cv2.imread(f'{TRAIN_DATA_PATH}/{image_id}', cv2.IMREAD_COLOR).copy().astype(np.float32)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n            image /= 255.0\n            if self.transforms:\n                sample = {'image': image}\n                sample = self.transforms(**sample)\n                image = sample['image']\n            return image, image_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class DatasetRetriever(Dataset):\n#     def __init__(self, image_ids, transforms=None):\n#         ids = pd.read_csv(LABELS).id.values\n#         self.image_ids = image_ids\n#         self.transforms = transforms\n        \n#     def __len__(self) -> int:\n#         return self.image_ids.shape[0]\n\n#     def __getitem__(self, index: int):\n#         image_id = self.image_ids[index]\n#         image = cv2.imread(f'{TRAIN_DATA_PATH}/{image_id}', cv2.IMREAD_COLOR).copy().astype(np.float32)\n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n#         image /= 255.0\n#         if self.transforms:\n#             sample = {'image': image}\n#             sample = self.transforms(**sample)\n#             image = sample['image']\n#         return image, image_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset = DatasetRetriever(\n#     image_ids=np.array([path.split('/')[-1] for path in glob(f'{TRAIN_DATA_PATH}/*'+IMAGE_NAME_EXTENSION)]),\n#     transforms=transform\n# )\n\n# def collate_fn(batch):\n#     return tuple(zip(*batch))\n\n# data_loader = DataLoader(\n#     dataset,\n#     batch_size=BATCH_SIZE[0],\n#     shuffle=True,\n#     num_workers=NUM_WORKERS,\n#     drop_last=DROP_LAST,\n#     collate_fn=collate_fn\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for epoch in range(2):  # loop over the dataset multiple times\n\n#     running_loss = 0.0\n#     for i, data in enumerate(trainloader, 0):\n#         # get the inputs; data is a list of [inputs, labels]\n#         inputs, labels = data[0].to(DEVICE), data[1].to(DEVICE)\n\n#         # zero the parameter gradients\n#         optimizer.zero_grad()\n\n#         # forward + backward + optimize\n#         outputs = model(inputs)\n#         loss = criterion(outputs, labels)\n#         loss.backward()\n#         optimizer.step()\n\n#         # print statistics\n#         running_loss += loss.item()\n#         if i % 2000 == 1999:    # print every 2000 mini-batches\n#             print('[%d, %5d] loss: %.3f' %\n#                   (epoch + 1, i + 1, running_loss / 2000))\n#             running_loss = 0.0\n\n# print('Finished Training')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # ==== TRAIN LOOP\n# tr_it = iter(train_dataloader)\n\n# progress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\n# losses_train = []\n\n# for itr in progress_bar:\n\n#     try:\n#         data = next(tr_it)\n#     except StopIteration:\n#         tr_it = iter(train_dataloader)\n#         data = next(tr_it)\n\n#     model.train()\n#     torch.set_grad_enabled(True)\n    \n#     # Forward pass\n#     inputs = data[\"image\"].to(device)\n#     target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n#     targets = data[\"target_positions\"].to(device)\n    \n#     outputs = model(inputs).reshape(targets.shape)\n#     loss = criterion(outputs, targets)\n\n#     # not all the output steps are valid, but we can filter them out from the loss using availabilities\n#     loss = loss * target_availabilities\n#     loss = loss.mean()\n\n#     # Backward pass\n#     optimizer.zero_grad()\n#     loss.backward()\n#     optimizer.step()\n\n#     losses_train.append(loss.item())\n\n#     if (itr+1) % cfg['train_params']['checkpoint_every_n_steps'] == 0 and not DEBUG:\n#         torch.save(model.state_dict(), f'model_state_{itr}.pth')\n    \n#     progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train[-100:])}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.save(model.state_dict(), TRAIN_MODEL_PATH)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. 混淆矩陣<a class=\"anchor\" id=\"8\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"markdown","source":"# 9. 提交<a class=\"anchor\" id=\"9\"></a>\n[Back to Table of Contents](#0)","metadata":{}},{"cell_type":"code","source":"# dataiter = iter(testloader)\n# images, labels = dataiter.next()\n\n# # print images\n# imshow(torchvision.utils.make_grid(images))\n# print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = build_model()\n# model.load_state_dict(torch.load(PATH))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# outputs = model(images)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# _, predicted = torch.max(outputs, 1)\n\n# print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n#                               for j in range(4)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correct = 0\n# total = 0\n# with torch.no_grad():\n#     for data in testloader:\n#         images, labels = data\n#         outputs = model(images)\n#         _, predicted = torch.max(outputs.data, 1)\n#         total += labels.size(0)\n#         correct += (predicted == labels).sum().item()\n\n# print('Accuracy of the network on the 10000 test images: %d %%' % (\n#     100 * correct / total))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class_correct = list(0. for i in range(10))\n# class_total = list(0. for i in range(10))\n# with torch.no_grad():\n#     for data in testloader:\n#         images, labels = data\n#         outputs = model(images)\n#         _, predicted = torch.max(outputs, 1)\n#         c = (predicted == labels).squeeze()\n#         for i in range(4):\n#             label = labels[i]\n#             class_correct[label] += c[i].item()\n#             class_total[label] += 1\n\n\n# for i in range(10):\n#     print('Accuracy of %5s : %2d %%' % (\n#         classes[i], 100 * class_correct[i] / class_total[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}