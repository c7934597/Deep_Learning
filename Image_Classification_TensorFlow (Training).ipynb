{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image_Classification_TensorFlow (Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0\"></a>\n",
    "# Table of Contents\n",
    "\n",
    "1. [套件安裝與載入](#1)\n",
    "1. [環境檢測與設定](#2)\n",
    "1. [開發參數設定](#3)\n",
    "1. [資料處理](#4)\n",
    "    -  [載入CSV檔](#4.1)\n",
    "    -  [檢查CSV檔缺失值](#4.2)\n",
    "1. [定義模型方法](#5)\n",
    "1. [定義回調函數方法](#6)\n",
    "1. [製作資料集＆資料擴增&訓練模型](#7)\n",
    "1. [混淆矩陣](#8)\n",
    "1. [待辦事項](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OHBv2EvpuAdt"
   },
   "source": [
    "# 1. 套件安裝與載入<a class=\"anchor\" id=\"1\"></a>\n",
    "[Back to Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q efficientnet\n",
    "import efficientnet.tfkeras as efn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-addons\n",
    "# import tensorflow_addons.optimizers as addons_optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 836,
     "status": "ok",
     "timestamp": 1601091312174,
     "user": {
      "displayName": "林韋銘",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8gCml3CniK-OQaF2rCP5B69FikKtFMoMLMJzLlQ=s64",
      "userId": "01731559530221412377"
     },
     "user_tz": -480
    },
    "id": "2iEt7NMlbO2o"
   },
   "outputs": [],
   "source": [
    "# 資料處理套件\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1134,
     "status": "ok",
     "timestamp": 1601091312478,
     "user": {
      "displayName": "林韋銘",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8gCml3CniK-OQaF2rCP5B69FikKtFMoMLMJzLlQ=s64",
      "userId": "01731559530221412377"
     },
     "user_tz": -480
    },
    "id": "d62NU4jDvL-z"
   },
   "outputs": [],
   "source": [
    "# 設定顯示中文字體\n",
    "from matplotlib.font_manager import FontProperties\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei'] # 用來正常顯示中文標籤\n",
    "plt.rcParams['font.family'] = 'AR PL UMing CN'\n",
    "plt.rcParams['axes.unicode_minus'] = False # 用來正常顯示負號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2569,
     "status": "ok",
     "timestamp": 1601091313918,
     "user": {
      "displayName": "林韋銘",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8gCml3CniK-OQaF2rCP5B69FikKtFMoMLMJzLlQ=s64",
      "userId": "01731559530221412377"
     },
     "user_tz": -480
    },
    "id": "rrxaxxw3vgD2"
   },
   "outputs": [],
   "source": [
    "# tensorflow深度學習模組套件\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.losses as losses\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "import tensorflow.keras.applications as applications\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 環境檢測與設定<a class=\"anchor\" id=\"2\"></a>\n",
    "[Back to Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3535,
     "status": "ok",
     "timestamp": 1601091314901,
     "user": {
      "displayName": "林韋銘",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8gCml3CniK-OQaF2rCP5B69FikKtFMoMLMJzLlQ=s64",
      "userId": "01731559530221412377"
     },
     "user_tz": -480
    },
    "id": "bhoGwFinSnp2",
    "outputId": "5dc9ff9d-cd0a-488d-b897-6228d44d458e"
   },
   "outputs": [],
   "source": [
    "# 查看設備\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3530,
     "status": "ok",
     "timestamp": 1601091314902,
     "user": {
      "displayName": "林韋銘",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8gCml3CniK-OQaF2rCP5B69FikKtFMoMLMJzLlQ=s64",
      "userId": "01731559530221412377"
     },
     "user_tz": -480
    },
    "id": "j3hkqlhuwa_h",
    "outputId": "3a161fa2-9263-4e59-a1de-6dda2e6e71a3"
   },
   "outputs": [],
   "source": [
    "# 查看tensorflow版本\n",
    "print(tf.__version__)\n",
    "\n",
    "# 查看圖像通道位置\n",
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''執行環境參數設定'''\n",
    "\n",
    "# (Boolean)是否為本機\n",
    "LOCAL = True\n",
    "\n",
    "# (Boolean)是否為 Colab\n",
    "COLAB = False\n",
    "\n",
    "# (String)CPU/GPU/TPU\n",
    "DEVICE = \"GPU\"\n",
    "\n",
    "\n",
    "'''檔案路徑參數設定'''\n",
    "\n",
    "# (String)Root路徑\n",
    "if LOCAL:\n",
    "    PATH = r'../'\n",
    "elif COLAB:\n",
    "    PATH = r'/content/drive/My Drive/Colab Notebooks/'\n",
    "else:\n",
    "    PATH = r'../input/'\n",
    "    OUTPUT_PATH = r'/kaggle/working/'\n",
    "    \n",
    "# (String)資料根路徑\n",
    "DATA_ROOT_PATH = PATH+r'datasets/AI_CUP_2020_AIMango_Defective_Classification/' \n",
    "\n",
    "# (String)訓練資料路徑\n",
    "TRAIN_DATA_PATH = DATA_ROOT_PATH+r'C1-P2_Train Dev/Train'\n",
    "\n",
    "# (String)訓練CSV路徑，如為None則不讀CSV檔\n",
    "TRAIN_CSV_PATH = DATA_ROOT_PATH+r'C1-P2_Train Dev/train.csv'\n",
    "\n",
    "# (String)專案名稱\n",
    "PROJECT_NAME = 'AI_CUP_2020_AIMango_Defective_Classification'\n",
    "\n",
    "# (String)專案檔案儲存路徑\n",
    "if LOCAL or COLAB:\n",
    "    OUTPUT_PATH = PATH\n",
    "PROJECT_PATH = OUTPUT_PATH+PROJECT_NAME+'/'+PROJECT_NAME+' '+datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "# (String)權重名稱(使用哪個權重)\n",
    "WEIGHTS_NAME = 'efficientnetb7'\n",
    "\n",
    "# (String)模型名稱(使用哪個模型)\n",
    "MODEL_NAME = 'efficientnetb7'\n",
    "\n",
    "# (String)讀取預訓練權重的儲存路徑 \n",
    "LOAD_WEIGHTS_PATH = PROJECT_PATH+r'/models/backup/'+WEIGHTS_NAME+'.h5'\n",
    "\n",
    "# (String)讀取預訓練模型的儲存路徑 \n",
    "LOAD_MODEL_PATH = PROJECT_PATH+r'/models/backup/'+MODEL_NAME+'.h5'\n",
    "\n",
    "# (String)CSV的儲存路徑\n",
    "CSV_SAVE_PATH = PROJECT_PATH+r'/csv/'+MODEL_NAME+'.csv'\n",
    "\n",
    "# (String)TensorBoard logs的儲存路徑 %tensorboard --logdir logs/fit\n",
    "TENSORBOARD_LOGS_PATH = PROJECT_PATH+r'/logs/fit'\n",
    "\n",
    "# (String)訓練模型的儲存路徑\n",
    "TRAIN_MODEL_PATH = PROJECT_PATH+r'/models/'+MODEL_NAME+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3526,
     "status": "ok",
     "timestamp": 1601091314903,
     "user": {
      "displayName": "林韋銘",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8gCml3CniK-OQaF2rCP5B69FikKtFMoMLMJzLlQ=s64",
      "userId": "01731559530221412377"
     },
     "user_tz": -480
    },
    "id": "CdnVnTy-Sr_m",
    "outputId": "6fa0d3af-d387-4617-c32a-63e8fdf01e04"
   },
   "outputs": [],
   "source": [
    "if not LOCAL and COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2560,
     "status": "ok",
     "timestamp": 1601091313920,
     "user": {
      "displayName": "林韋銘",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8gCml3CniK-OQaF2rCP5B69FikKtFMoMLMJzLlQ=s64",
      "userId": "01731559530221412377"
     },
     "user_tz": -480
    },
    "id": "pf5jZ5kXSumb",
    "outputId": "a61ca438-c8a8-4812-b20c-18d2406acebe"
   },
   "outputs": [],
   "source": [
    "if DEVICE != \"CPU\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3520,
     "status": "ok",
     "timestamp": 1601091314903,
     "user": {
      "displayName": "林韋銘",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8gCml3CniK-OQaF2rCP5B69FikKtFMoMLMJzLlQ=s64",
      "userId": "01731559530221412377"
     },
     "user_tz": -480
    },
    "id": "C1N2eFZK4ZBe",
    "outputId": "49851184-e88a-4439-9198-7abfc24e2032"
   },
   "outputs": [],
   "source": [
    "if DEVICE == \"TPU\":\n",
    "    print(\"connecting to TPU...\")\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        print(\"Could not connect to TPU\")\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        try:\n",
    "            print(\"initializing  TPU ...\")\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "            print(\"TPU initialized\")\n",
    "        except _:\n",
    "            print(\"failed to initialize TPU\")\n",
    "    else:\n",
    "        DEVICE = \"GPU\"\n",
    "\n",
    "if DEVICE != \"TPU\":\n",
    "    print(\"Using default strategy for CPU and single GPU\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "if DEVICE == \"GPU\":\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    \n",
    "\n",
    "AUTO     = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動態申請顯存\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(TRAIN_CSV_PATH):\n",
    "    LOAD_CSV = True\n",
    "else:\n",
    "    LOAD_CSV = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(PROJECT_PATH+r'/models/'):\n",
    "    os.makedirs(PROJECT_PATH+r'/models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7iXLpmwwfPw"
   },
   "source": [
    "# 3. 開發參數設定<a class=\"anchor\" id=\"3\"></a>\n",
    "[Back to Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2565,
     "status": "ok",
     "timestamp": 1601091313919,
     "user": {
      "displayName": "林韋銘",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8gCml3CniK-OQaF2rCP5B69FikKtFMoMLMJzLlQ=s64",
      "userId": "01731559530221412377"
     },
     "user_tz": -480
    },
    "id": "K7AfQ-_3whDv",
    "outputId": "6ca09974-e10a-4d81-fa48-08d3da015039"
   },
   "outputs": [],
   "source": [
    "'''客製參數設定'''\n",
    "\n",
    "\n",
    "'''資料參數設定'''\n",
    "\n",
    "# (Int)分類數量\n",
    "CLASSES = 3\n",
    "\n",
    "# (Int)有CSV檔該參數才有用，1則為不做交叉驗證\n",
    "FOLD = 1\n",
    "\n",
    "# (Int)沒CSV檔，FOLD該參數固定為1\n",
    "if not LOAD_CSV:\n",
    "    FOLD = 1\n",
    "\n",
    "# (Int)圖片尺寸\n",
    "IMAGE_SIZE = [224]*FOLD\n",
    "\n",
    "# (String)圖片副檔名\n",
    "IMAGE_NAME_EXTENSION = '.jpg'\n",
    "\n",
    "# (String)CSV圖片檔名欄位(不包含路徑)\n",
    "IMAGE_NAME = 'image_id'\n",
    "\n",
    "# (String)CSV圖片檔名欄位(包含路徑)\n",
    "IMAGE_NAME_ROOT = 'image'\n",
    "\n",
    "# (String)CSV標籤欄位\n",
    "LABEL_NAME = 'grade'\n",
    "\n",
    "# (String)CSV標籤欄位類型\n",
    "LABEL_NAME_TYPE = 'string'\n",
    "\n",
    "# (Boolean)CSV圖片檔名欄位是否包含副檔名\n",
    "IMAGE_NAME_HAVE_EXTENSION = True\n",
    "\n",
    "#  (String)預設：'rgb'，圖像是否轉換為 1 個或 3 個顏色通道\n",
    "COLOR_MODE = 'rgb'\n",
    "\n",
    "#  (String)預設：'categorical'，決定返回標籤數組的類型：\"categorical\" 將是 2D one-hot 編碼標籤\n",
    "CLASS_MODE = 'categorical'\n",
    "\n",
    "# (Int)不同的種子會產生不同的Random或分層K-FOLD分裂, 42則是預設固定種子\n",
    "SEED = 42\n",
    "\n",
    "if FOLD == 1:\n",
    "    # (Float)驗證集佔訓練集的比率，FOLD>1則不啟用\n",
    "    DATA_SPLIT = 0.2\n",
    "else:\n",
    "    # (String)切分訓練集跟驗證集方式\n",
    "    SKF = StratifiedKFold(n_splits=FOLD,shuffle=True,random_state=SEED)\n",
    "\n",
    "\n",
    "'''資料擴增參數設定'''\n",
    "\n",
    "# (Boolean)對輸入數據施加ZCA白化\n",
    "ZCA_WHITENING = False\n",
    "\n",
    "# (Float)數據提升時圖片隨機轉動的角度\n",
    "ROTATION_RANGE = 15.0\n",
    "\n",
    "# (Float)圖片寬度的某個比例，數據提升時圖片水平偏移的幅度\n",
    "WIDTH_SHIFT_RANGE = 0.2\n",
    "\n",
    "# (Float)圖片高度的某個比例，數據提升時圖片水平偏移的幅度\n",
    "HEIGHT_SHIFT_RANGE = 0.2\n",
    "\n",
    "# (Float)剪切強度（逆時針方向的剪切變換角度）\n",
    "SHEAR_RANGE = 0.2\n",
    "\n",
    "# (Float)隨機縮放的幅度\n",
    "ZOOM_RANGE = 0.2\n",
    "\n",
    "# (Boolean)進行隨機水平翻轉\n",
    "HORIZONTAL_FILP = True\n",
    "\n",
    "# (Boolean)進行隨機垂直翻轉\n",
    "VERTICAL_FILP = False\n",
    "\n",
    "# (String)預設︰'nearest'，當進行變換時超出邊界的點將根據本參數給定的方法進行處理\n",
    "FILL_MODE = 'nearest'\n",
    "\n",
    "\n",
    "''''模型參數設定'''\n",
    "\n",
    "# (Boolean)使用TF模型，如為False則須客制另外撰寫\n",
    "USE_BASE_MODEL = True\n",
    "\n",
    "if USE_BASE_MODEL:\n",
    "    # (Boolean)使用EFFICIENTNET模型\n",
    "    USE_EFFICIENTNET_MODEL = True\n",
    "    if USE_EFFICIENTNET_MODEL:\n",
    "        # (Int List)使用哪一種EFFICIENTNET\n",
    "        EFF_NET = [0]*FOLD\n",
    "\n",
    "        # (Model List)列出每種縮放尺寸的EFFICIENTNET\n",
    "        BASE_MODEL = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n",
    "                efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7]\n",
    "    else:\n",
    "        # (Model)建立TF模型\n",
    "        BASE_MODEL = applications.MobileNetV2\n",
    "\n",
    "# (Boolean)是否使用TF權重\n",
    "LOAD_TF_WEIGHTS = True\n",
    "\n",
    "if LOAD_TF_WEIGHTS:\n",
    "    # (String)TF預訓練權重為 imagenet/noisy-student\n",
    "    WEIGHTS = 'imagenet'\n",
    "else:\n",
    "    WEIGHTS = None\n",
    "\n",
    "# (Boolean)TF模型是否包含完全連接網路頂部的網路層\n",
    "INCLUDE_TOP = False\n",
    "    \n",
    "# (Boolean)TF模型是否可訓練權重(不包括頂部網路層)\n",
    "BASE_MODEL_TRAINABLE = True\n",
    "\n",
    "# (Boolean)是否已有客製模型，僅載入權重\n",
    "LOAD_WEIGHTS = False\n",
    "\n",
    "# (Boolean)是否載入完整客製(模型+權重)\n",
    "LOAD_MODEL = False\n",
    "\n",
    "# (Float)Dropout比率 0.5\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# 最後輸出時的激活函數，雙分類為sigmoid，多分類為softmax\n",
    "ACTIVATION_FUNCTION = 'softmax'\n",
    "\n",
    "# (Boolean)是否印出完整模型\n",
    "MODEL_PRINT = False\n",
    "\n",
    "# (Boolean)是否儲存模型圖表\n",
    "SAVE_MODEL_DIAGRAM = True\n",
    "\n",
    "if SAVE_MODEL_DIAGRAM:\n",
    "    # (Boolean)是否儲存模型圖表網路形狀\n",
    "    SAVE_MODEL_SHAPE = True\n",
    "    \n",
    "    # (String)儲存模型圖表完整檔名\n",
    "    SAVE_MODEL_FILENAME = PROJECT_PATH+r'/models/'+MODEL_NAME+'.png'\n",
    "\n",
    "\n",
    "''''回調函數參數設定'''\n",
    "\n",
    "# (Boolean)回調函數 ModelCheckpoint / ParallelModelCheckpoint 是否啟用\n",
    "CALLBACKS_CHECK_POINTER = True\n",
    "\n",
    "# (Boolean)回調函數 EarlyStoppin 是否啟用\n",
    "CALLBACKS_EARLY_STOPPING = False\n",
    "\n",
    "# (Boolean)回調函數 CSVLogger 是否啟用\n",
    "CALLBACKS_CSV_LOGGER = False\n",
    "\n",
    "# (Boolean)回調函數 LearningRateScheduler 是否啟用\n",
    "CALLBACKS_LR_SCHEDULER = True\n",
    "\n",
    "# (Boolean)回調函數 ReduceLROnPlateau 是否啟用\n",
    "CALLBACKS_REDUCE_LR = True\n",
    "\n",
    "# (Boolean)回調函數 TensorBoard 是否啟用\n",
    "CALLBACKS_TENSOR_BOARD = False\n",
    "\n",
    "# (String)回調函數監控數值 acc/val_acc/loss/val_loss\n",
    "MONITOR = 'val_loss'\n",
    "\n",
    "# (Boolean)回調函數 ModelCheckpoint / ParallelModelCheckpoint 是否只儲存最佳模型 False\n",
    "SAVE_BEST_ONLY = True\n",
    "\n",
    "# (Boolean)回調函數 ModelCheckpoint / ParallelModelCheckpoint 是否只儲存權重 False\n",
    "SAVE_WEIGHTS_ONLY = False\n",
    "\n",
    "# (Int)回調函數 EarlyStopping 沒有改善的時期數，之後訓練將停止 10\n",
    "PATIENCE_ELS = 10\n",
    "\n",
    "# (Boolean)回調函數 CSVLogger True為是否接下去原有CSV檔，False為覆蓋 \n",
    "APPEND = True\n",
    "\n",
    "# (Float)回調函數 LearningRateSchedule 初始學習率 0.000003\n",
    "LEARNING_RATE_START = 0.000003\n",
    "\n",
    "# (Float)回調函數 LearningRateSchedule 最大學習率 0.000020\n",
    "LEARNING_RATE_MAX = 0.000020\n",
    "\n",
    "# (Float)回調函數 LearningRateSchedule 最小學習率 0.000001\n",
    "LEARNING_RATE_MIN = 0.000001\n",
    "\n",
    "# (Float)回調函數 LearningRateSchedule 學習率多少時期後開始上升 5\n",
    "LEARNING_RATE_RAMPUP_EPOCHS = 5\n",
    "\n",
    "# (Float)回調函數 LearningRateSchedule 學習率支撐時期 0\n",
    "LEARNING_RATE_SUSTAIN_EPOCHS = 0\n",
    "\n",
    "# (Float)回調函數 LearningRateSchedule 學習率衰減 0.8\n",
    "LEARNING_RATE_EXP_DECAY = 0.8\n",
    "\n",
    "# (Float)回調函數 ReduceLROnPlateau 被降低的因數。新的學習速率 = 學習速率 * 因數 0.2\n",
    "FACTOR = 0.2\n",
    "\n",
    "# (Int)回調函數 ReduceLROnPlateau 沒有進步的訓練輪數，在這之後訓練速率會被降低 5 \n",
    "PATIENCE_RLR = 5\n",
    "\n",
    "# (Float)回調函數 ReduceLROnPlateau 最小學習率 0\n",
    "LEARNING_RATE_MIN_RLR = 0\n",
    "\n",
    "# (String)回調函數 TensorBoard 'batch'或'epoch'或整數。使用時'batch'，每批之後將損失和指標寫入TensorBoard\n",
    "# 同樣適用於'epoch'。如果使用整數，假設1000，回調將每1000批將指標和損失寫入TensorBoard\n",
    "UPDATE_FREQ = 'epoch'\n",
    "\n",
    "# (Boolean)回調函數 TensorBoard 是否在TensorBoard中可視化圖形。當write_graph設置為True時，日誌文件可能會變得很大\n",
    "WRITE_GRAPH = True\n",
    "\n",
    "# (Int/String)回調函數 TensorBoard 分析批次以採樣計算特徵。profile_batch必須是非負整數或整數元組。一對正整數表示要分析的批次範圍\n",
    "# 默認情況下，它將配置第二批。將profile_batch = 0設置為禁用分析 5 / '10,20'\n",
    "PROFILE_BATCH = 5\n",
    "\n",
    "\n",
    "''''編譯參數設定'''\n",
    "\n",
    "if not CALLBACKS_LR_SCHEDULER:\n",
    "    # (Float)優化器學習率 1e-3/1e-1\n",
    "    LEARNING_RATE = None\n",
    "    \n",
    "    # (Float)優化器權重衰減 5e-5/5e-4\n",
    "    WEIGHT_DECAY = None\n",
    "\n",
    "# (Float)加速優化器在相關方向上前進，並抑制震盪 0.9\n",
    "MOMENTUM = None\n",
    "\n",
    "# (Float)優化器模糊因子比率 1e-7\n",
    "EPSILON = None\n",
    "\n",
    "# (String)優化器指定，None為客制，須另外撰寫\n",
    "BASE_OPTIMIZERS = optimizers.Adam()\n",
    "\n",
    "# (Float)標籤平滑比率，將one-hot的編碼方式變得更加soft 0.1\n",
    "LABEL_SMOOTHING = 0\n",
    "\n",
    "# (String)損失函數，None為客制，須另外撰寫\n",
    "BASE_LOSSES = None\n",
    "\n",
    "# (Float)focusing parameter for modulating factor (1-p)\n",
    "LOSSES_GAMMA = 2.\n",
    "\n",
    "# (Float)the same as weighing factor in balanced cross entropy\n",
    "LOSSES_ALPHA = .25\n",
    "\n",
    "# (String List)評價指標，None為客制，須另外撰寫\n",
    "BASE_METRICS = ['accuracy']\n",
    "\n",
    "# (String )評價指標的圖表顯示\n",
    "PLOT_METRICS = 'accuracy'\n",
    "\n",
    "\n",
    "''''訓練參數設定'''\n",
    "\n",
    "# (Int List)每批訓練的尺寸\n",
    "BATCH_SIZE = [16]*FOLD\n",
    "\n",
    "# (Int)訓練做幾次時代\n",
    "EPOCHS = [2]*FOLD\n",
    "\n",
    "# (Int)從多少時代開始訓練\n",
    "INITIAL_EPOCH = 0\n",
    "\n",
    "# (Int)日誌顯示，0為靜音，1為進度條，2為顯示每個紀錄\n",
    "VERBOSE = 1\n",
    "\n",
    "# (Int)使用基於進程的線程時，要啟動的最大進程數。如果未指定，workers則默認為1。如果為0，將在主線程上執行生成器\n",
    "WORKERS = 1\n",
    "\n",
    "# (Boolean)如果為True，請使用基於多進程的線程。如果未指定，則默認為False\n",
    "# 請注意，由於此實現依賴於多處理，因此不應將不可拾取的參數傳遞給生成器，因為它們無法輕易傳遞給子進程\n",
    "USE_MULTIPROCESS = False\n",
    "\n",
    "# (Boolean) 是否使用class_weight來平衡訓練標籤的分配不均，可能會降低準確率，不過會提昇真實標籤的正確性。\n",
    "USE_ClASS_WEIGHT = True\n",
    "\n",
    "\n",
    "''''圖表參數設定'''\n",
    "\n",
    "# (Float)全部SNS圖表的字形縮放\n",
    "ALL_SNS_FONT_SCALE = 1.0\n",
    "\n",
    "# (Int)CSV缺失值圖表寬度\n",
    "CSV_COUNTPLOT_FIGSIZE_W = 10\n",
    "\n",
    "# (Int)CSV缺失值圖表高度\n",
    "CSV_COUNTPLOT_FIGSIZE_H = 10\n",
    "\n",
    "# (Int)CSV缺失值圖表標題字型大小\n",
    "CSV_COUNTPLOT_TITLE_FONTSIZE = 20\n",
    "\n",
    "# (Int)CSV缺失值圖表X軸標題字型大小\n",
    "CSV_COUNTPLOT_XLABEL_FONTSIZE = 15\n",
    "\n",
    "# (Int)CSV缺失值圖表Y軸標題字型大小\n",
    "CSV_COUNTPLOT_YLABEL_FONTSIZE = 15\n",
    "\n",
    "# (Int)訓練歷程圖表寬度\n",
    "TRAINING_CURVES_FIGSIZE_W = 20\n",
    "\n",
    "# (Int)訓練歷程圖表高度\n",
    "TRAINING_CURVES_FIGSIZE_H = 10\n",
    "\n",
    "# (Int)訓練歷程圖表SCATTER的標記點大小 \n",
    "TRAINING_CURVES_SCATTER_SCALAR = 200\n",
    "\n",
    "# (Float)訓練歷程圖表SCATTER的指標文字離標記點X距離係數\n",
    "TRAINING_CURVES_SCATTER_METRICS_TEXT_XSCALAR = 0.03\n",
    "\n",
    "# (Float)訓練歷程圖表SCATTER的指標文字標記點Y距離係數\n",
    "TRAINING_CURVES_SCATTER_METRICS_TEXT_YSCALAR = 0.13\n",
    "\n",
    "# (Float)訓練歷程圖表SCATTER的損失文字標記點X距離係數\n",
    "TRAINING_CURVES_SCATTER_LOSS_TEXT_XSCALAR = 0.03\n",
    "\n",
    "# (Float)訓練歷程圖表SCATTER的損失文字標記點Y距離係數\n",
    "TRAINING_CURVES_SCATTER_LOSS_TEXT_YSCALAR = 0.05\n",
    "\n",
    "# (Int)訓練歷程圖表SCATTER的文字大小\n",
    "TRAINING_CURVES_SCATTER_TEXTSIZE = 15\n",
    "\n",
    "# (Int)訓練歷程圖表X軸標題字型大小\n",
    "TRAINING_CURVES_XLABEL_FONTSIZE = 15\n",
    "\n",
    "# (Int)訓練歷程圖表Y軸標題字型大小\n",
    "TRAINING_CURVES_YLABEL_FONTSIZE = 15\n",
    "\n",
    "# (Int)訓練歷程圖表標題字型大小\n",
    "TRAINING_CURVES_TITLE_FONTSIZE = 20\n",
    "\n",
    "# (Float)訓練歷程圖表格線粗度\n",
    "TRAINING_CURVES_GRID_ALPHA = 0\n",
    "\n",
    "# (Int)混淆矩陣圖表寬度\n",
    "CONFUSION_MATRIX_FIGSIZE_W = 10\n",
    "\n",
    "# (Int)混淆矩陣圖表高度\n",
    "CONFUSION_MATRIX_FIGSIZE_H = 10\n",
    "\n",
    "# (Int)混淆矩陣圖表內容字型大小\n",
    "CONFUSION_MATRIX_HEATMAP_FONTSIZE = 15\n",
    "\n",
    "# (Int)混淆矩陣圖表標題字型大小\n",
    "CONFUSION_MATRIX_TITLE_FONTSIZE = 20\n",
    "\n",
    "# (Int)混淆矩陣圖表X軸標題字型大小\n",
    "CONFUSION_MATRIX_XLABEL_FONTSIZE = 15\n",
    "\n",
    "# (Int)混淆矩陣圖表Y軸標題字型大小\n",
    "CONFUSION_MATRIX_YLABEL_FONTSIZE = 15\n",
    "\n",
    "# (String)預設'binary'：僅在目標為二進制，兩分類時適用。\n",
    "#'micro'：通過計算總的真陽性，假陰性和假陽性來全局計算指標，多分類時適用。\n",
    "#'macro'：計算每個標籤的指標，並找到其未加權平均值。這沒有考慮標籤不平衡，多分類時適用。\n",
    "AVERAGE = 'macro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設置sns圖表縮放係數\n",
    "sns.set(font_scale = ALL_SNS_FONT_SCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 資料處理<a class=\"anchor\" id=\"4\"></a>\n",
    "[Back to Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 載入CSV檔 <a class=\"anchor\" id=\"4.1\"></a>\n",
    "[Back to Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_CSV:\n",
    "    print('Reading data...')\n",
    "\n",
    "    # 讀取訓練資料集CSV檔\n",
    "    train_csv = pd.read_csv(TRAIN_CSV_PATH,encoding=\"utf8\")\n",
    "\n",
    "    print('Reading data completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_CSV:\n",
    "    # 顯示訓練資料集CSV檔\n",
    "    print(train_csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_CSV:\n",
    "    print(\"Shape of train_data :\", train_csv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 檢查CSV檔缺失值 <a class=\"anchor\" id=\"4.2\"></a>\n",
    "[Back to Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_CSV:\n",
    "    total = train_csv.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (train_csv.isnull().sum()/train_csv.isnull().count()*100).sort_values(ascending = False)\n",
    "    missing_train_csv  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    print(missing_train_csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_CSV:\n",
    "    print(train_csv[LABEL_NAME].value_counts())\n",
    "    f,ax = plt.subplots(figsize=(CSV_COUNTPLOT_FIGSIZE_W, CSV_COUNTPLOT_FIGSIZE_H))\n",
    "    sns.countplot(train_csv[LABEL_NAME], hue = train_csv[LABEL_NAME],ax = ax)\n",
    "    plt.title(\"LABEL COUNT\", fontsize=CSV_COUNTPLOT_TITLE_FONTSIZE)\n",
    "    plt.xlabel(LABEL_NAME.upper(), fontsize=CSV_COUNTPLOT_XLABEL_FONTSIZE)\n",
    "    plt.ylabel(\"COUNT\", fontsize=CSV_COUNTPLOT_YLABEL_FONTSIZE)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 定義模型方法<a class=\"anchor\" id=\"5\"></a>\n",
    "[Back to Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "\n",
    "def categorical_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Softmax version of focal loss.\n",
    "           m\n",
    "      FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
    "          c=1\n",
    "      where m = number of classes, c = class and o = observation\n",
    "    Parameters:\n",
    "      alpha -- the same as weighing factor in balanced cross entropy\n",
    "      gamma -- focusing parameter for modulating factor (1-p)\n",
    "    Default value:\n",
    "      gamma -- 2.0 as mentioned in the paper\n",
    "      alpha -- 0.25 as mentioned in the paper\n",
    "    References:\n",
    "        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
    "    Usage:\n",
    "     model.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def categorical_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred: A tensor resulting from a softmax\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # Scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "\n",
    "        # Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Calculate Cross Entropy\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "        # Calculate Focal Loss\n",
    "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "\n",
    "        # Sum the losses in mini_batch\n",
    "        return K.sum(loss, axis=1)\n",
    "\n",
    "    return categorical_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizers():\n",
    "    if BASE_OPTIMIZERS == None:\n",
    "        print(\"Custiom OPTIMIZERS\")\n",
    "    else:\n",
    "        RETURN_OPTIMIZERS = BASE_OPTIMIZERS\n",
    "    return RETURN_OPTIMIZERS\n",
    "\n",
    "optimizer = build_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_losses():\n",
    "    if BASE_LOSSES == None:\n",
    "        if CLASSES == 2:\n",
    "            RETURN_LOSSES = binary_focal_loss(gamma = LOSSES_GAMMA, alpha = LOSSES_ALPHA)\n",
    "        else:\n",
    "            RETURN_LOSSES = categorical_focal_loss(gamma = LOSSES_GAMMA, alpha = LOSSES_ALPHA)\n",
    "        print(\"Custiom LOSSES\")\n",
    "    else:\n",
    "        RETURN_LOSSES = BASE_LOSSES\n",
    "    return RETURN_LOSSES\n",
    "\n",
    "loss = build_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_metrics():\n",
    "    if BASE_METRICS == None:\n",
    "        print(\"Custiom METRICS\")\n",
    "    else:\n",
    "        RETURN_METRICS = BASE_METRICS\n",
    "    return RETURN_METRICS\n",
    "\n",
    "metrics = build_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dim = 256, fold = 0):\n",
    "    if LOAD_MODEL:\n",
    "        print(\"Reading the pre-trained model... \")\n",
    "        model = keras.models.load_model(LOAD_MODEL_PATH)\n",
    "        print(\"Reading done. \")\n",
    "    else:\n",
    "        if USE_BASE_MODEL:\n",
    "            if USE_EFFICIENTNET_MODEL:\n",
    "                base_model = BASE_MODEL[EFF_NET[fold]](input_shape=(dim,dim,3), include_top=INCLUDE_TOP, weights=WEIGHTS)\n",
    "                for layer in base_model.layers:\n",
    "                    layer.trainable = BASE_MODEL_TRAINABLE\n",
    "                inputs = base_model.inputs[0]\n",
    "                outputs = base_model.outputs[0]\n",
    "                x = layers.AveragePooling2D(name=\"averagepooling2d_head\")(outputs)\n",
    "                x = layers.Flatten(name=\"flatten_head\")(x)\n",
    "                x = layers.Dense(64, activation=\"relu\", name=\"dense_head\")(x)\n",
    "                x = layers.Dropout(DROPOUT, name=\"dropout_head\")(x)\n",
    "            else:\n",
    "                base_model = BASE_MODEL(input_shape =(dim, dim, 3), include_top=INCLUDE_TOP, weights=WEIGHTS)\n",
    "                for layer in base_model.layers:\n",
    "                    layer.trainable = BASE_MODEL_TRAINABLE\n",
    "                inputs = base_model.inputs[0]\n",
    "                outputs = base_model.outputs[0]\n",
    "                x = layers.GlobalAveragePooling2D(name=\"globalaveragepooling2d_head\")(outputs)\n",
    "                x = layers.Dropout(DROPOUT, name=\"dropout_head\")(x)\n",
    "        else:\n",
    "            print(\"Custiom Model\")\n",
    "        outputs = layers.Dense(CLASSES, activation=ACTIVATION_FUNCTION, name=\"predictions_head\")(x)                                    \n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n",
    "\n",
    "        if MODEL_PRINT:\n",
    "            model.summary()\n",
    "        if SAVE_MODEL_DIAGRAM:\n",
    "            plot_model(model, show_shapes= SAVE_MODEL_SHAPE, to_file=SAVE_MODEL_FILENAME)\n",
    "\n",
    "        if LOAD_WEIGHTS :\n",
    "            print(\"Reading the pre-trained weights... \")\n",
    "            model.load_weights(LOAD_WEIGHTS_PATH)\n",
    "            print(\"Reading done. \")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 定義回調函數方法<a class=\"anchor\" id=\"6\"></a>\n",
    "[Back to Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(checkpointpath):\n",
    "    check_pointer = callbacks.ModelCheckpoint(filepath = checkpointpath, monitor = MONITOR, verbose = VERBOSE, \n",
    "                                              save_best_only = SAVE_BEST_ONLY)\n",
    "\n",
    "    # Interrupt the training when the validation loss is not decreasing\n",
    "    early_stopping = callbacks.EarlyStopping(monitor = MONITOR, verbose = VERBOSE, patience = PATIENCE_ELS)\n",
    "\n",
    "    # Stream each epoch results into a .csv file\n",
    "    csv_logger = callbacks.CSVLogger(CSV_SAVE_PATH, separator = ',', append = APPEND)\n",
    "\n",
    "    # LEARNING RATE SCHEDULER\n",
    "    def get_lr_callback():\n",
    "        def lrfn(epoch):\n",
    "            if epoch < LEARNING_RATE_RAMPUP_EPOCHS:\n",
    "                lr = (LEARNING_RATE_MAX - LEARNING_RATE_START) / LEARNING_RATE_RAMPUP_EPOCHS * epoch + LEARNING_RATE_START\n",
    "\n",
    "            elif epoch < LEARNING_RATE_RAMPUP_EPOCHS + LEARNING_RATE_SUSTAIN_EPOCHS:\n",
    "                lr = LEARNING_RATE_MAX\n",
    "\n",
    "            else:\n",
    "                lr = (LEARNING_RATE_MAX - LEARNING_RATE_MIN) * LEARNING_RATE_EXP_DECAY**(epoch - LEARNING_RATE_RAMPUP_EPOCHS - LEARNING_RATE_SUSTAIN_EPOCHS) + LEARNING_RATE_MIN\n",
    "\n",
    "            return lr\n",
    "\n",
    "        lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=VERBOSE)\n",
    "        return lr_callback\n",
    "\n",
    "    lr_scheduler = get_lr_callback()\n",
    "\n",
    "    # Reduce learning rate when a metric has stopped improving\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor = MONITOR, verbose = VERBOSE, factor = FACTOR, patience = PATIENCE_RLR, \n",
    "                                  min_lr = LEARNING_RATE_MIN_RLR)\n",
    "\n",
    "    tensor_board = callbacks.TensorBoard(log_dir = TENSORBOARD_LOGS_PATH, update_freq = UPDATE_FREQ, write_graph = WRITE_GRAPH, \n",
    "                               profile_batch = PROFILE_BATCH)\n",
    "\n",
    "    callbacks_list = []\n",
    "    if CALLBACKS_CHECK_POINTER:\n",
    "        callbacks_list.append(check_pointer)\n",
    "    if CALLBACKS_EARLY_STOPPING:\n",
    "        callbacks_list.append(early_stopping)\n",
    "    if CALLBACKS_CSV_LOGGER:\n",
    "        callbacks_list.append(csv_logger)\n",
    "    if CALLBACKS_LR_SCHEDULER:\n",
    "        callbacks_list.append(lr_scheduler)\n",
    "    if CALLBACKS_REDUCE_LR:\n",
    "        callbacks_list.append(reduce_lr)\n",
    "    if CALLBACKS_TENSOR_BOARD:\n",
    "        callbacks_list.append(tensor_board)\n",
    "    \n",
    "    return callbacks_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 製作資料集＆資料擴增&訓練模型 <a class=\"anchor\" id=\"7\"></a>\n",
    "[Back to Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 宣告為訓練後預測用\n",
    "all_labels = []; all_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_training_curves(history, fold, skf):\n",
    "    plt.figure(figsize=(TRAINING_CURVES_FIGSIZE_W,TRAINING_CURVES_FIGSIZE_H))\n",
    "    plt.plot(np.arange(EPOCHS[fold]),history.history[PLOT_METRICS],'-o',label='TRAIN '+PLOT_METRICS.upper(),color='#ff7f0e')\n",
    "    plt.plot(np.arange(EPOCHS[fold]),history.history['val_'+PLOT_METRICS],'-o',label='VALIDATION '+PLOT_METRICS.upper(),color='#1f77b4')\n",
    "    x = np.argmax( history.history['val_'+PLOT_METRICS] ); y = np.max( history.history['val_'+PLOT_METRICS] )\n",
    "    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "    plt.scatter(x,y,s=TRAINING_CURVES_SCATTER_SCALAR,color='#1f77b4')\n",
    "    plt.text(x-TRAINING_CURVES_SCATTER_METRICS_TEXT_XSCALAR*xdist,y-TRAINING_CURVES_SCATTER_METRICS_TEXT_YSCALAR*ydist,'max '+PLOT_METRICS+'\\n%.4f'%y,size=TRAINING_CURVES_SCATTER_TEXTSIZE)\n",
    "    plt.ylabel(PLOT_METRICS.upper(),size=TRAINING_CURVES_YLABEL_FONTSIZE); plt.xlabel('EPOCH',size=TRAINING_CURVES_XLABEL_FONTSIZE)\n",
    "    plt.grid(alpha=TRAINING_CURVES_GRID_ALPHA)\n",
    "    plt.legend(loc=2)\n",
    "    plt2 = plt.gca().twinx()\n",
    "    plt2.plot(np.arange(EPOCHS[fold]),history.history['loss'],'-o',label='TRAIN LOSS',color='#2ca02c')\n",
    "    plt2.plot(np.arange(EPOCHS[fold]),history.history['val_loss'],'-o',label='VALIDATION LOSS',color='#d62728')\n",
    "    x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n",
    "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "    plt.scatter(x,y,s=TRAINING_CURVES_SCATTER_SCALAR,color='#d62728')\n",
    "    plt.text(x-TRAINING_CURVES_SCATTER_LOSS_TEXT_XSCALAR*xdist,y+TRAINING_CURVES_SCATTER_LOSS_TEXT_YSCALAR*ydist,'min loss\\n%.4f'%y,size=TRAINING_CURVES_SCATTER_TEXTSIZE)\n",
    "    plt.ylabel('LOSS',size=TRAINING_CURVES_YLABEL_FONTSIZE)\n",
    "    if skf:\n",
    "        plt.title('FOLD %i - IMAGE SIZE %i, %s'%\n",
    "                  (fold+1, IMAGE_SIZE[fold], MODEL_NAME.upper()), size=TRAINING_CURVES_TITLE_FONTSIZE)\n",
    "    else:\n",
    "        plt.title(' IMAGE SIZE %i, %s'%\n",
    "                  (IMAGE_SIZE[fold], MODEL_NAME.upper()), size=TRAINING_CURVES_TITLE_FONTSIZE)\n",
    "    plt.grid(alpha=TRAINING_CURVES_GRID_ALPHA)\n",
    "    plt.legend(loc=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_process(fold, skf, x_train, x_val, y_train, y_val):\n",
    "    if skf:\n",
    "        print('FOLD %i - IMAGE SIZE %i WITH %s AND BATCH_SIZE %i'%(fold+1,IMAGE_SIZE[fold],MODEL_NAME.upper(),BATCH_SIZE[fold]*REPLICAS))\n",
    "    else:\n",
    "        print('IMAGE SIZE %i WITH %s AND BATCH_SIZE %i'%(IMAGE_SIZE[fold],MODEL_NAME.upper(),BATCH_SIZE[fold]*REPLICAS))\n",
    "        \n",
    "    if LOAD_CSV:\n",
    "        train_data = pd.DataFrame(x_train)\n",
    "        train_data.columns = [IMAGE_NAME_ROOT]\n",
    "        train_data[LABEL_NAME] = y_train\n",
    "\n",
    "        validation_data = pd.DataFrame(x_val)\n",
    "        validation_data.columns = [IMAGE_NAME_ROOT]\n",
    "        validation_data[LABEL_NAME] = y_val\n",
    "\n",
    "        train_data[LABEL_NAME] = train_data[LABEL_NAME].astype(LABEL_NAME_TYPE)\n",
    "        validation_data[LABEL_NAME] = validation_data[LABEL_NAME].astype(LABEL_NAME_TYPE)\n",
    "\n",
    "        train_datagen = ImageDataGenerator(rescale = 1. / 255,\n",
    "                                           zca_whitening = ZCA_WHITENING,\n",
    "                                           rotation_range = ROTATION_RANGE,\n",
    "                                           width_shift_range = WIDTH_SHIFT_RANGE,\n",
    "                                           height_shift_range = HEIGHT_SHIFT_RANGE,\n",
    "                                           shear_range = SHEAR_RANGE,\n",
    "                                           zoom_range = ZOOM_RANGE,\n",
    "                                           horizontal_flip = HORIZONTAL_FILP,\n",
    "                                           vertical_flip = VERTICAL_FILP,\n",
    "                                           fill_mode = FILL_MODE)\n",
    "\n",
    "        val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "        train_generator = train_datagen.flow_from_dataframe(\n",
    "            train_data,\n",
    "            x_col = IMAGE_NAME_ROOT,\n",
    "            y_col = LABEL_NAME,\n",
    "            target_size = (IMAGE_SIZE[fold],IMAGE_SIZE[fold]),\n",
    "            batch_size = BATCH_SIZE[fold],\n",
    "            shuffle = True,\n",
    "            seed = SEED,\n",
    "            color_mode = COLOR_MODE,\n",
    "            class_mode = CLASS_MODE)\n",
    "\n",
    "        validation_generator = val_datagen.flow_from_dataframe(\n",
    "            validation_data,\n",
    "            x_col = IMAGE_NAME_ROOT,\n",
    "            y_col = LABEL_NAME,\n",
    "            target_size = (IMAGE_SIZE[fold], IMAGE_SIZE[fold]),\n",
    "            batch_size = BATCH_SIZE[fold],\n",
    "            shuffle = False,\n",
    "            seed = SEED,\n",
    "            color_mode = COLOR_MODE,\n",
    "            class_mode = CLASS_MODE)\n",
    "    else:\n",
    "        train_datagen = ImageDataGenerator(rescale = 1. / 255,\n",
    "                                           zca_whitening = ZCA_WHITENING,\n",
    "                                           rotation_range = ROTATION_RANGE,\n",
    "                                           width_shift_range = WIDTH_SHIFT_RANGE,\n",
    "                                           height_shift_range = HEIGHT_SHIFT_RANGE,\n",
    "                                           shear_range = SHEAR_RANGE,\n",
    "                                           zoom_range = ZOOM_RANGE,\n",
    "                                           horizontal_flip = HORIZONTAL_FILP,\n",
    "                                           vertical_flip = VERTICAL_FILP,\n",
    "                                           fill_mode = FILL_MODE,\n",
    "                                           validation_split = DATA_SPLIT)\n",
    "        \n",
    "        validation_datagen = ImageDataGenerator(rescale = 1. / 255,\n",
    "                                           validation_split = DATA_SPLIT)\n",
    "\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "                    TRAIN_DATA_PATH, \n",
    "                    target_size = (IMAGE_SIZE[fold], IMAGE_SIZE[fold]), \n",
    "                    batch_size = BATCH_SIZE[fold], \n",
    "                    shuffle = True,\n",
    "                    seed = SEED,\n",
    "                    color_mode = COLOR_MODE, \n",
    "                    class_mode = CLASS_MODE, \n",
    "                    subset = 'training')\n",
    "\n",
    "        validation_generator = validation_datagen.flow_from_directory(\n",
    "                    TRAIN_DATA_PATH, \n",
    "                    target_size = (IMAGE_SIZE[fold], IMAGE_SIZE[fold]), \n",
    "                    batch_size = BATCH_SIZE[fold], \n",
    "                    shuffle = False,\n",
    "                    seed = SEED,\n",
    "                    color_mode = COLOR_MODE, \n",
    "                    class_mode = CLASS_MODE, \n",
    "                    subset = 'validation')\n",
    "        \n",
    "    # (Int)聲明一個紀元完成並開始下一個紀元之前的總步數(一批樣品)\n",
    "    STEPS_PER_EPOCH = tf.math.ceil(train_generator.n/train_generator.batch_size/REPLICAS)\n",
    "    # (Int)在每個時期結束時執行驗證時，在停止之前要繪製的步驟總數(樣本批次)\n",
    "    VALIDATION_STEPS = tf.math.ceil(validation_generator.n/validation_generator.batch_size/REPLICAS)\n",
    "    print(\"Number of training and validation steps: {} and {}\".format(STEPS_PER_EPOCH,VALIDATION_STEPS))\n",
    "     \n",
    "    # BUILD MODEL\n",
    "    K.clear_session()\n",
    "    with strategy.scope():\n",
    "        model = build_model(dim = IMAGE_SIZE[fold], fold = fold)\n",
    "     \n",
    "    if LOAD_CSV and skf:\n",
    "        # (String)訓練模型FOLD>1的儲存路徑\n",
    "        SAVE_MODEL_PATH = PROJECT_PATH+r'/models/'+MODEL_NAME+'_fold_%i.h5'%(fold+1)\n",
    "    else:\n",
    "        SAVE_MODEL_PATH = TRAIN_MODEL_PATH\n",
    "        \n",
    "    print(\"train batch \", train_generator.__getitem__(0)[0].shape)\n",
    "    print(\"validation batch \", validation_generator.__getitem__(0)[0].shape)\n",
    "    print(\"sample train label \\n\", train_generator.__getitem__(0)[1][:5])\n",
    "    print(\"sample validation label \\n\", validation_generator.__getitem__(0)[1][:5])\n",
    "    print(\"train class indices \", train_generator.class_indices)\n",
    "    print(\"validation class indices \", validation_generator.class_indices)\n",
    "        \n",
    "    counter = Counter(train_generator.classes)\n",
    "    max_val = float(max(counter.values()))\n",
    "    class_weight = {class_id : max_val/num_images for class_id, num_images in counter.items()}\n",
    "    print('calss weight ', class_weight)\n",
    "    if not USE_ClASS_WEIGHT:\n",
    "        class_weight = None\n",
    "        \n",
    "    print('Training...')\n",
    "    history = model.fit(\n",
    "        train_generator, \n",
    "        steps_per_epoch = STEPS_PER_EPOCH, \n",
    "        epochs = EPOCHS[fold], \n",
    "        callbacks = get_callbacks(checkpointpath = SAVE_MODEL_PATH), \n",
    "        validation_data = validation_generator, \n",
    "        validation_steps = VALIDATION_STEPS, \n",
    "        verbose = VERBOSE, \n",
    "        workers = WORKERS, \n",
    "        use_multiprocessing = USE_MULTIPROCESS, \n",
    "        initial_epoch = INITIAL_EPOCH, \n",
    "        class_weight = class_weight)\n",
    "    print('Training done!')\n",
    "    \n",
    "    if CALLBACKS_CHECK_POINTER:\n",
    "        model.load_weights(TRAIN_MODEL_PATH)\n",
    "    else:\n",
    "        model.save(TRAIN_MODEL_PATH)\n",
    "        \n",
    "    display_training_curves(history, fold, skf)\n",
    "        \n",
    "    all_labels.append(validation_generator.classes)\n",
    "    all_pred.append(np.argmax(model.predict(validation_generator), axis = -1))\n",
    "    \n",
    "    if LOAD_CSV:\n",
    "        del x_train, x_val, y_train, y_val\n",
    "    del train_data, validation_data\n",
    "    del train_generator, validation_generator, history\n",
    "    if LOAD_CSV and skf:\n",
    "        del model\n",
    "    K.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_CSV:\n",
    "    label_list = []\n",
    "    train_list = []\n",
    "    for i in range(train_csv.shape[0]):\n",
    "        if IMAGE_NAME_HAVE_EXTENSION:\n",
    "            train_list.append(TRAIN_DATA_PATH + '/' + train_csv[IMAGE_NAME].iloc[i])\n",
    "        else:\n",
    "            train_list.append(TRAIN_DATA_PATH + '/' + train_csv[IMAGE_NAME].iloc[i] + IMAGE_NAME_EXTENSION)\n",
    "        label_list.append(train_csv[LABEL_NAME].iloc[i])\n",
    "    df_train = pd.DataFrame(train_list)\n",
    "    df_train.columns = [IMAGE_NAME_ROOT]\n",
    "    df_train[LABEL_NAME] = label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if LOAD_CSV and FOLD == 1:\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(df_train[IMAGE_NAME_ROOT],df_train[LABEL_NAME], test_size = DATA_SPLIT, random_state = SEED)\n",
    "    train_process(fold = 0, skf = False, x_train = X_train, x_val = X_val, y_train = Y_train, y_val = Y_val)\n",
    "    del X_train, X_val, Y_train, Y_val\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "elif LOAD_CSV and FOLD > 1:\n",
    "    for fold,(train_index, val_index) in enumerate(SKF.split(df_train[IMAGE_NAME_ROOT], df_train[LABEL_NAME])):\n",
    "        X_train, X_val = df_train[IMAGE_NAME_ROOT].iloc[train_index], df_train[IMAGE_NAME_ROOT].iloc[val_index]\n",
    "        Y_train, Y_val = df_train[LABEL_NAME].iloc[train_index], df_train[LABEL_NAME].iloc[val_index]\n",
    "        train_process(fold = fold, skf = True, x_train = X_train, x_val = X_val, y_train = Y_train, y_val = Y_val)\n",
    "        del X_train, X_val, Y_train, Y_val\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "else:\n",
    "    train_process(fold = 0, skf = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 混淆矩陣<a class=\"anchor\" id=\"8\"></a>\n",
    "[Back to Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_correct_labels = np.concatenate(all_labels)\n",
    "cm_predictions = np.concatenate(all_pred)\n",
    "print(\"Correct   labels: \", cm_correct_labels.shape, cm_correct_labels)\n",
    "print(\"Predicted labels: \", cm_predictions.shape, cm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''混淆矩陣包含四個要素:TP(True Positive)正確預測成功的正樣本, TN(True Negative)正確預測成功的負樣本, \n",
    "FP(False Positive)錯誤預測成正樣本，實際上為負樣本, FN(False Negative)錯誤預測成負樣本(或者說沒能預測出來的正樣本)'''\n",
    "cm = tf.math.confusion_matrix(cm_correct_labels, cm_predictions)\n",
    "cm = cm/cm.numpy().sum(axis=1)[:, tf.newaxis]\n",
    "\n",
    "f,ax = plt.subplots(figsize = (CONFUSION_MATRIX_FIGSIZE_W, CONFUSION_MATRIX_FIGSIZE_H))\n",
    "sns.heatmap(cm, annot = True, ax = ax, annot_kws={\"size\": CONFUSION_MATRIX_HEATMAP_FONTSIZE})\n",
    "plt.title(\"CONFUSION MATRIX\", fontsize=CONFUSION_MATRIX_TITLE_FONTSIZE)\n",
    "plt.xlabel(\"PREDICTED\", fontsize=CONFUSION_MATRIX_XLABEL_FONTSIZE)\n",
    "plt.ylabel(\"TRUE\", fontsize=CONFUSION_MATRIX_YLABEL_FONTSIZE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "Precision(準確率) = TP/(TP+FP)\n",
    "Recall(召回率) = TP/(TP+FN)\n",
    "F1-score(Recall與Precision的調和平均數) = 2 * Precision * Recall / (Precision + Recall)\n",
    "'''\n",
    "accuracy = accuracy_score(cm_correct_labels, cm_predictions)\n",
    "precision = precision_score(cm_correct_labels, cm_predictions, labels = range(CLASSES), average = AVERAGE)\n",
    "recall = recall_score(cm_correct_labels, cm_predictions, labels = range(CLASSES), average = AVERAGE)\n",
    "score = f1_score( cm_correct_labels, cm_predictions, labels = range(CLASSES), average = AVERAGE)\n",
    "print('Accuracy: {:.3f}, Precision: {:.3f}, Recall: {:.3f}, F1 score: {:.3f}'.format(accuracy, precision, recall, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report on training Data\n",
    "print(classification_report(cm_correct_labels, cm_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 待辦事項<a class=\"anchor\" id=\"9\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to Top](#0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Face Recognition.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}